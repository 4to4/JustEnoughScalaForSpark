{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Enough Scala for Spark\n",
    "\n",
    "Dean Wampler, Ph.D. [@deanwampler](http://twitter.com/deanwampler) ([email](mailto:dean.wampler@lightbend.com))\n",
    "\n",
    "Welcome. This notebook teaches you the core concepts of [Scala](http://scala-lang.org) necessary to use [Apache Spark's](http://spark.apache.org) Scala API effectively. Spark does a nice job exploiting the nicest features of Scala, while avoiding most of the more difficult and obscure features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scala?\n",
    "Spark lets you use Scala, Java, Python, R, and SQL to do your work. Scala and Java appeal to _data engineers_, who do the heavy lifting of building resilient and scalable infrastructures for _Big Data_. Python, R, and SQL appeal to _data scientists_, who build models for analyzing data, including machine learning, as well explore data interactively, where SQL is very convenient.\n",
    "\n",
    "These aren't hard boundaries. Many people do both roles. Many data engineers like Python and may use SQL and R. Many data scientists have decided to use Scala with Spark.\n",
    "\n",
    "Briefly, some of the advantages of using Scala include the following:\n",
    "* Since Spark is written in Scala, you get the best performance and the most complete API coverage when you use Scala. It's true that with [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html), code written in all five languages performs about the same. If you need to use the [RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) API, then Scala provides the best performance, with Java a close second.\n",
    "* When runtime problems occur, understanding the exception stack frames and other debug information is easiest if you know Scala. Unfortunately, the \"abstraction leaks\" when problems occur.\n",
    "* Compared to Java, Scala code is much more concise and several features of Scala make your code even more concise. This elevates your productivity and makes it easier to imagine a design approach and then write it down without having to translate to much to idiomatic API and language conventions. (You'll see this in action as we go.)\n",
    "* Compared to Python and R, Scala code benefits from _static typing_ with _type inference_. _Static typing_ means that the Scala parser finds more errors in your expressions, when they don't match expected types, rather than discovering the problem when you actually run the code. However, _type inference_ means you don't have to add a lot of explicit type information to you code. In many cases, Scala infers the correct types.\n",
    "\n",
    "But Scala isn't perfect. The biggest disadvantage compared to Python and R is the rich ecosystem of data analytics libraries available in those languages. Scala is trying to catch up, but Python and R are still well ahead.\n",
    "\n",
    "Obviously, I can only scratch the surface of Scala here. For more information:\n",
    "* [Programming Scala](http://shop.oreilly.com/product/0636920033073.do): My comprehensive introduction to Scala.\n",
    "* [Scala Language Website](http://scala-lang.org/): Where to download Scala, find documentation (e.g., the [Scaladocs](http://www.scala-lang.org/api/current/#package): Scala library documentation, like [Javadocs](https://docs.oracle.com/javase/8/docs/api/)), and other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Notebooks\n",
    "You're using the [Jupyter Notebook](http://jupyter.org/) environment with [Apache Toree](https://toree.incubator.apache.org/) to provide a Spark backend. The [README](README.md) explains how to set up the environment.\n",
    "\n",
    "Notebooks let you mix documentation, like this [Markdown](https://daringfireball.net/projects/markdown/) \"cell\", with cells that contain code, like the next cell. \n",
    "\n",
    "The menus and toolbar at the top provide options for evaluating a cell, adding and deleting cells, etc. You'll want to learn keyboard shortcuts if you use notebooks a lot. I recommend that you invoke the _Help > Keyboard Shortcuts_ menu item, then capture the page as an image (it's a modal dialog, unfortunately). Learn a few shortcuts each day.\n",
    "\n",
    "For now, just note that when you're in a cell, `shift+enter` evaluates the cell (parses and renders the Markdown or evaluates the code), then moves to the next cell. Try it for a few cells. I'll wait...\n",
    "\n",
    "Okay. Note that you can click into any cell to move the focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure Toree to always show us the types of expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "When you start this notebook, Toree creates a [SparkContext](http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark) for you. This is the entry point of any Spark application. It knows how to connect to your cluster (or run locally in the same JVM), how to configure properties, etc. It also runs a Web UI that lets you monitor your running jobs. The instance of `SparkContext` is called `sc`. The next cell simply confirms that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext = org.apache.spark.SparkContext@7898967"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get more information about the environment and statistics about the jobs you've run or are currently running from the Spark UI: <a href=\"http://localhost:4040\" target=\"spark_ui\">http://localhost:4040</a> (Opens in a new tab.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Download Some Data\n",
    "We're going to write a real Spark program and use it as a vehicle for learning Scala and how to use it with Spark. \n",
    "\n",
    "But first, we need to download some text files we'll use, which contain some of the plays of Shakespeare. The next few cells define some helper methods (functions) to do this and then perform the download. So, we'll also learn some basic Scala concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE: \"method\" vs. \"function\"**\n",
    "\n",
    "> Scala follows a common object-oriented convention where the term _method_ is used for a function that's attached to a class or instance. Unlike Java, at least before Java 8, Scala also has _functions_ that are not associated with a particular class or instance. \n",
    "\n",
    "> In our next code example, we'll define a _method_ that becomes part of a generated class that the Scala interpreter creates to wrap all our notebook code. The interpreter has to do this in order to generate valid JVM byte code. We'll see what a real _function_ looks likes like soon.\n",
    "\n",
    "> Unfortunately, it can be a bit confusing when to use a method vs. a function, reflecting Scala's hybrid nature as an object-oriented and a functional language. Fortunately, in many cases, we can use methods and functions interchangably, so we won't worry about the distinction too much from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's define two convenience _methods_ for printing either an error message or a simple \"information\" message. We'll explain the syntax in the cell that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    " * \"error\" takes a single String argument, prints a formatted error message,\n",
    " * and returns false. \n",
    " */\n",
    "def error(message: String): Boolean = {   \n",
    "    \n",
    "    // Print the string passed to \"println\" and add a linefeed (\"ln\"):\n",
    "    // See the next cell for an explanation of how the string is constructed.\n",
    "    println(s\"\"\"\n",
    "        |********************************************************************\n",
    "        |\n",
    "        |  ERROR: $message\n",
    "        |\n",
    "        |********************************************************************\n",
    "        |\"\"\".stripMargin)\n",
    "    \n",
    "    // The last expression in a block, {...}, is the return value. \n",
    "    // The \"return\" keyword is not needed.\n",
    "    false\n",
    "}\n",
    "\n",
    "/*\n",
    " * \"info\" takes a single String argument, prints it on a line,\n",
    " * and returns true. \n",
    " */\n",
    "def info(message: String): Boolean = {\n",
    "    println(message)\n",
    "    true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method definitions have the following elements, in order:\n",
    "* The `def` keyword.\n",
    "* The method's name (`error` and `info` in these cases).\n",
    "* The argument list in parentheses. If there are no arguments, the empty parentheses can be omitted. This is common for \"getter\"-like methods that simply return a field in an instance, etc.\n",
    "* A colon followed by the type of the value returned by the method. (This can often be inferred, so it's optional - but recommended for readibility - in those cases).\n",
    "* An `=` (equals) sign that separates the method _signature_ from the _body_.\n",
    "* The body in braces `{ ... }`, although if the body consists of a single expression, the braces are optional.\n",
    "* The last expression in the body is used as the return value. The `return` keyword is optional and rarely used.\n",
    "* Like `return`, semicolons (`;`) are inferred at the end of lines (in most cases) and rarely used.\n",
    "\n",
    "In the argument list for `error`, `(message: String)`, `message` is the argument name and its type is `String`. This convention for _type annotations_, `: Type`, is also used for the return type, `error(...): Boolean`. Type annotations are required by Scala for method arguments. They are optional in most cases for the return type. We'll see that Scala can infer the types of many expressions and variable declarations.\n",
    "\n",
    "Scala uses the same comment conventions as Java, `// ...` for a single line, and `/* ... */` for a comment block.\n",
    "\n",
    "> An _expression_ has a value, while a _statement_ does not. Hence, when we assign an expression to a variable, the value the expression returns is assigned to the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `error`, we used a combined _interpolated_ and _triple-quoted_ string, with the syntax `s\"\"\"...\"\"\"`:\n",
    "* Triple-quoted strings, `\"\"\"...\"\"\"`, can embed newlines, as in the example. (We'll see another benefit later.)\n",
    "* String interpolation, invoked by putting `s` before the string, e.g., `s\"...\"` or `s\"\"\"...\"\"\"`, lets us embed variable references and expressions, where the string conversion will be inserted automatically. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String = Use braces for expressions: Some(deanwampler).\n",
       "You can omit the braces when just using a variable: org.apache.spark.SparkContext@7898967\n",
       "However, watch for ambiguities like org.apache.spark.SparkContext@7898967andextrastuff"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s\"\"\"Use braces for expressions: ${sys.env.get(\"USER\")}.\n",
    "You can omit the braces when just using a variable: $sc\n",
    "However, watch for ambiguities like ${sc}andextrastuff\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature we're using for triple-quoted strings is the ability to strip the leading whitespace off each line. The `stripMargin` method removes all whitespace before and including the `|`. This lets you indent those lines for proper code formatting, but not have that whitespace remain in the string. In the following example, the resulting string has blank lines at the beginning and end. Also note what happens with `line2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String = \"\n",
       "line 1\n",
       "  line 2\n",
       "\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s\"\"\"\n",
    "    |line 1\n",
    "    |  line 2\n",
    "    |\"\"\".stripMargin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a method that works similar to the popularity [curl](http://linux.die.net/man/1/curl) utility. It's a bit long and you don't need to understand all the details, but we'll need it to download data we need for the notebook.\n",
    "\n",
    "The whole body is one big [scala.util.Try](http://www.scala-lang.org/api/current/#scala.util.Try) expression. `Try` is a useful class for \"trying\" code that might throw an exception (more precisely, a [java.lang.Throwable](https://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html)). The long block, that is `Try {...}`, ends with `outFile`, a `File` instance. This is the result returned by the block, if everything succeeds. This `File` is wrapped in a `Try` subclass instance of type [scala.util.Success](http://www.scala-lang.org/api/current/scala/util/Success.html). However, if an exception is thrown in the block, then the exception is wrapped in an instance of another subclass, [scala.util.Failure](http://www.scala-lang.org/api/current/scala/util/Failure.html). (We'll discuss other advantages of `Try` and alternatives to it <a href=\"#TryOptionNull\">here</a>.)\n",
    "\n",
    "Another construct we'll see for the first time is how to declare variables:\n",
    "* `val immutableValue = ...`: Once initialized, we can't assign a _different_ value to `immutableValue`.\n",
    "* `var mutableVariable = ...`: We can assign new values to `mutableVariable` as often as we want.\n",
    "\n",
    "It's _highly recommended_ that you only use `vals` unless you have a good reason for needing mutability, which is a very common source of bugs!!\n",
    "\n",
    "Most of the types used here are from the Java's library (JDK). Because Scala compiles to JVM byte code, you can use any Java library you want from Scala:\n",
    "* [java.net.URL](https://docs.oracle.com/javase/8/docs/api/java/net/URL.html): Handles URL formatting and connections.\n",
    "* [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html): Working with files and directories.\n",
    "* [java.io.BufferedInputStream](https://docs.oracle.com/javase/8/docs/api/java/io/BufferedInputStream.html): Buffered input from an underlying stream.\n",
    "* [java.io.BufferedOutputStream](https://docs.oracle.com/javase/8/docs/api/java/io/BufferedOutputStream.html): Buffered output to an underlying stream.\n",
    "* [java.io.FileOutputStream](https://docs.oracle.com/javase/8/docs/api/java/io/FileOutputStream.html): Output to a file, specifically.\n",
    "\n",
    "As before, we'll use comments again to explain a few other new Scala constructs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Import this utility for working with URLs. Unlike Java the semicolon ';' is not required.\n",
    "import java.net.URL   \n",
    "\n",
    "// Use {...} to provide a list of things to import, when you don't want to import everything \n",
    "// in a package (_ in Scala) and you don't want to write a separate line for each one.\n",
    "import java.io.{File, BufferedInputStream, BufferedOutputStream, FileOutputStream}\n",
    "\n",
    "import scala.util.Try\n",
    "\n",
    "// Download a file at a URL and write it to a target directory:\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): Try[File] = Try {\n",
    "\n",
    "    // The path separator on your platform: \"/\" on Linux and MacOS, \"\\\" on Windows.\n",
    "    val pathSeparator = File.separator\n",
    "\n",
    "    // Use the name of the remote file as the file name in the target directory.\n",
    "    // We split on the URL path elements using the separator, which is ALWAYS \"/\"\n",
    "    // on all platforms.\n",
    "    val sourceFileName = sourceURLString.split(\"/\").last  \n",
    "    val outFileName = targetDirectoryString + pathSeparator + sourceFileName\n",
    "\n",
    "    println(s\"Downloading $sourceURLString to $outFileName\")\n",
    "    val sourceURL = new URL(sourceURLString)\n",
    "    val connection = sourceURL.openConnection()\n",
    "    val in = new BufferedInputStream(connection.getInputStream()) // Used to read the bytes.\n",
    "\n",
    "    // If here, the connection was successfully opened (i.e., no exception was thrown).\n",
    "    // Now create the target directory (nothing happens if already there).\n",
    "    val targetDirectory = new File(targetDirectoryString)\n",
    "    targetDirectory.mkdirs()\n",
    "\n",
    "    // Setup the output file and a stream to write to it.\n",
    "    val outFile = new File(outFileName)\n",
    "    val out = new BufferedOutputStream(new FileOutputStream(outFile))\n",
    "    \n",
    "    // Create the buffer into which we'll hold in-flight bytes.\n",
    "    val hundredK = 100*1024\n",
    "    val bytes = Array.fill[Byte](hundredK)(0)   // Create a byte buffer, elements set to 0\n",
    "\n",
    "    // Loop until we've read everything.\n",
    "    var loops = 0\n",
    "    var count = in.read(bytes, 0, hundredK)     // Read up to \"hundredK\" bytes at a time.\n",
    "    while (count != -1) {                       // Haven't hit the end of input yet?\n",
    "        if (loops % 10 == 0) print(\".\")         // Print occasional feedback.\n",
    "        loops += 1\n",
    "        out.write(bytes, 0, count)              // Write to the new file.\n",
    "        count = in.read(bytes, 0, hundredK)     // Read the next chunk and loop...\n",
    "    }\n",
    "    println(\"\\nFinished!\")\n",
    "    in.close()                                  // Clean up! Close file & stream handles\n",
    "    out.flush()\n",
    "    out.close()\n",
    "    outFile                                     // Returned file (if we got this far)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, before we actually use `curl`, let's create the target directory. (This is also done in `curl`, but we're using the success or failure for other purposes here.) \n",
    "\n",
    "Note that Scala's `if` construct is actually an expression (in Java they are _statements_). The `if` expression will return `true` or `false` and assign it to `success`, which we'll use in a moment. This is why both `error` and `info` return `Boolean`, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data/shakespeare\n"
     ]
    }
   ],
   "source": [
    "// The target directory, which we'll now create, if necessary.\n",
    "val shakespeare = new File(\"data/shakespeare\")\n",
    "\n",
    "val success = if (shakespeare.exists == false) {   // doesn't exist already?\n",
    "    if (shakespeare.mkdirs() == false) {           // did the attempt fail??\n",
    "        error(s\"Failed to create directory path: $shakespeare\")\n",
    "    } else {                                       // successful\n",
    "        info(s\"Created $shakespeare\")\n",
    "    }\n",
    "} else {\n",
    "    info(s\"$shakespeare already exists\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we successfully created the output directory (or it already existed), let's download a handful of files, each with one play of Shakespeare, from [http://www.cs.usyd.edu.au/~matty/Shakespeare/](http://www.cs.usyd.edu.au/~matty/Shakespeare/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pathSeparator = File.separator\n",
    "val targetDirName = shakespeare.toString\n",
    "val urlRoot = \"http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/\"\n",
    "val plays = Seq(\n",
    "    \"tamingoftheshrew\", \"comedyoferrors\", \"loveslabourslost\", \"midsummersnightsdream\",\n",
    "    \"merrywivesofwindsor\", \"muchadoaboutnothing\", \"asyoulikeit\", \"twelfthnight\")\n",
    "\n",
    "if (success) {\n",
    "    for {\n",
    "        play <- plays\n",
    "        playFile = new File(targetDirName + pathSeparator + play)\n",
    "        if (playFile.exists == false)\n",
    "        result = curl(urlRoot + play, targetDirName)\n",
    "    } {\n",
    "        // If successful, then `foreach` extracts the `File` so we can use it.\n",
    "        result.foreach(file => info(s\"Downloaded $play and wrote $file\"))\n",
    "        // If unsuccessful, `recover` extracts the `Throwable`...\n",
    "        result.recover{ case throwable => error(s\"Failed to download $play. $throwable\") }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use something called a `for` _comprehension_. They are _expressions_, not _statements_ as in Java's `for` loops, although we ignore the return value here. `for (play <- plays)` iterates through a collection, `plays`, and assigns each one to the `play` variable. Subsequent steps use it, first to create a `File` instance (`playFile`) and then to evaluate a conditional - do we already have this file downloaded?\n",
    "\n",
    "If we haven't downloaded it yet, the final expression in the `for` comprehension uses `curl` to download it. Recall that `curl` returns a `Try[File]`. We actually don't care about the `File`, but we do care about success or failure. Note how we process this in the second block:\n",
    "* `result.foreach` actually does nothing if a `Failure` is returned, but if a `Success` is returned, calling `foreach` extracts the `File` inside the `Success` and we use it for an information message.\n",
    "* `result.recover` does nothing if a `Success` is returned, but if a `Failure` is returned, the `Throwable` is extracted and we use it to print an error message. \n",
    "\n",
    "> For both `result.foreach` and `result.recover`, we pass an _anonymous function_ to them to do the processing we want. We'll explain this concept and the two kinds of syntax you see here in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use of a `for` comprehension is sometimes called a \"for loop\" because it returns nothing; it just prints messages. For completeness, here's another example, where we construct a new collection using a comprehension. We'll use it as a sanity check to verify we were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We should have 8 files:\n",
      "data/shakespeare/tamingoftheshrew       \tSuccess!\n",
      "data/shakespeare/comedyoferrors         \tSuccess!\n",
      "data/shakespeare/loveslabourslost       \tSuccess!\n",
      "data/shakespeare/midsummersnightsdream  \tSuccess!\n",
      "data/shakespeare/merrywivesofwindsor    \tSuccess!\n",
      "data/shakespeare/muchadoaboutnothing    \tSuccess!\n",
      "data/shakespeare/asyoulikeit            \tSuccess!\n",
      "data/shakespeare/twelfthnight           \tSuccess!\n"
     ]
    }
   ],
   "source": [
    "println(s\"We should have ${plays.size} files:\")\n",
    "val list = for {\n",
    "    play <- plays \n",
    "    playFileString = targetDirName + pathSeparator + play\n",
    "    playFile = new File(playFileString)\n",
    "} yield {\n",
    "    \"%-40s\\t%s\".format(playFileString, if (playFile.exists) \"Success!\" else \"NOT FOUND!!\")\n",
    "}\n",
    "list.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yield` keyword tells Scala that we want to construct a new collection, using the expression that follows to construct elements. Here we use another way to format a string, using C-style `printf` formatting. Because there is only one expression after the `yield`, the braces `{...}` are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index - When You're Tired of Counting Words...\n",
    "\n",
    "Whew! We've learned a lot of Scala already while doing typical data science chores (i.e., fetching data). \n",
    "\n",
    "Now let's implement a real algorithm using Spark, _Inverted Index_. You'll want this when you create your next \"Google killer\". It takes in a corpus of documents (e.g., web pages), tokenizes the words, and outputs for each word a list of the documents that contain it, along with the corresponding counts. \n",
    "\n",
    "This is a slightly more interesting algorithm than _Word Count_, the classic \"hello world\" program everyone implements when they learn Spark.\n",
    "\n",
    "The term _inverted_ here means we started with the words as part of the input _values_, where the _keys_ where the document identifiers, and we switched to using the words as keys and the document identifiers as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our first version, all at once. This is _one long expression_. Note the periods `.` at the end of the subexpressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiFirstPass1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll break it down into steps, assigning each step to a variable. This extra verbosity let's us see what Scala infers for the type returned by the expression. \n",
    "\n",
    "This is one of the nice features of Scala. We don't have to put in the type information ourselves, like we would have to do for Java code, but the compiler gives us feedback about what we just created. This is especially useful when you're learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[(String, String)] = data/shakespeare MapPartitionsRDD[81] at wholeTextFiles at <console>:37"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileContents = sc.wholeTextFiles(shakespeare.toString)\n",
    "fileContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line with `fileContents` is there so the notebook will show us its type information. (Try to remove it and re-evaluate the cell. Nothing is printed.). \n",
    "\n",
    "The output is telling us that `fileContents` has the type `RDD[(String,String)]`, but the instance is actually a `MapPartitionsRDD`, which is a \"private\" implementation subclass of `RDD`. Note that `RDD[...]` means there is a type parameter required in the brackets, `[...]`, which represents the type of records being held. \n",
    "\n",
    "The expression `(String,String)` is a convenient shorthand for `Tuple2[String,String]`. That is, we have two-element _tuples_ as records, where the first element is for each file's fully-qualified path and the second element is the contents of the file. This is what `SparkContext.wholetextFiles` returns for us. We'll use the path to remember where we found words, while the contents contains the words themselves, of course.\n",
    "\n",
    "We'll see shortly that you can also write _instances_ of [Tuple2](http://www.scala-lang.org/api/current/index.html#scala.Tuple2) with the same syntax, e.g., `(\"foo\", 101)`, for a `(String,Int)` tuple, and similarly for _higher-arity_ tuples (up to 22 elements...), e.g., `(\"foo\", 101, 3.14159, (\"bar\", 202L))`. Run the next cell to the type signature for this four-element tuple. Do you understand it? Do you understand why it has four elements and not five?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(String, Int, Double, (String, Long)) = (foo,101,3.14159,(bar,202))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"foo\", 101, 3.14159, (\"bar\", 202L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many `fileContents` records do we have? Not many. It should be the same number as the number of files we downloaded (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 8"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileContents.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our next step in the calculation, split the contents on non-alphanumeric characters (which also removes the newlines), extract the last element of the fully-qualified path, the file name, and return a new tuple.\n",
    "\n",
    "Our parsing is very crude. It improperly handles contractions, like `it's` and hyphenated words like `world-changing`. When you kill Google, be sure to use a real _natural language processing_ parsing technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[82] at flatMap at <console>:41"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordFileNameOnes = fileContents.flatMap { location_contents_tuple2 => \n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}\n",
    "wordFileNameOnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If find this hard to read and shortly I'll show you a much more elegant alternative syntax.\n",
    "\n",
    "First, if we called `fileContents.map`, it would return exactly _one_ new record for each record in _fileContents_. What we actually want now are records for each word-fileName combination, a significantly larger number (but the data in each record will be much smaller). For this we use `fileContents.flatMap`. In general, a `flatMap` returns a _collection_ of new records, zero or more, for _each_ input record. These collections are then _flattened_ into one big collection, another `RDD` in this case.\n",
    "\n",
    "What should `flatMap` actually do with each record? We saw earlier the syntax for defining _methods_. Now we see the syntax for _functions_, `argument_list => body`. We pass a literal function definition, without a name, so it's an _anonymous function_. \n",
    "\n",
    "Here, we have a single argument, the record, which we named `location_contents_tuple2`, a verbose way to say that it's a two-element tuple with an input file's location and contents. We don't require a type parameter for `location_contents_tuple2`. The `=>` \"arrow\" separates the argument list from the body, which appears on the next few lines.\n",
    "\n",
    "If we had more than one argument or we added the type _annotations_ (optional most of the time), then we would need parentheses, e.g.,:\n",
    "\n",
    "```scala\n",
    "(location_contents_tuple2: Tuple2[String,String]) => ...\n",
    "(arg1, arg2, arg3) => ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, I said we're passing a function as an argument to `flatMap`. If so, why are using braces `{...}` around this function instead of parentheses `(...)` like you would normally expect when passing arguments to a method like `flatMap`? \n",
    "\n",
    "It's because Scala lets us use braces instead of parentheses so we have the familiar block-like syntax we know and love for `if` and `for` expressions. You can use either braces or parentheses here, although the informal convention for a multi-line anonymous function is to use braces and to use parentheses for a single expression on the same line. (There's one exception where braces are required, which we'll see later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each `location_contents_tuple2`, we access the _first_ element using the `_1` method and the _second_ element using `_2`.\n",
    "\n",
    "So, we grab the `contents` in the second element split it using a _regular expression_ for non-alphanumeric characters. That returns an `Array` of words. For the first element, we extract the file name at the end of the location path. This isn't really necessary, but it makes the output more readable if we remove the common prefix from the path. \n",
    "Finally, we use `Array.map` in the Scala library, _not_ `RDD.map` to transform each `word` into tuple of the form `((word, fileName), 1)`.\n",
    "\n",
    "Why did we embed a tuple of `(word, fileName)` inside the \"outer\" tuple with a `1` as the second element? Why not just write a three-element tuple, `(word, fileName, 1)`? We will use the `(word, fileName)` as a _key_ in the next step, to find all unique word-fileName combinations, using the equivalent of a `group by` statement. The `1` is a \"seed\" count, which we'll use to count the occurrences of the unique `(word, fileName)` pairs.\n",
    "\n",
    "> **Notes:**\n",
    "> * For historical reasons, tuple indices start at 1, not 0 like arrays and other Scala collections.\n",
    "> * Another benefit of triple-quoted strings that makes them nice for regular expressions is that you don't have to escape regular expression metacharacters, like `\\W`. If we used a single-quoted string, we would have to write it as `\"\\\\W+\"`. Your choice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of records we have and look at a few of the lines. We'll use the `RDD.take` method to grab the first 10 lines, then loop over them and print them.\n",
    "\n",
    "> **Note:** In a cluster, the actually lines chosen will be the first lines from an arbitrarily-chosen _partition_ of the data, so they won't necessarily be the very first lines from the first file, alphabetically sorted, or even the first ten lines of the chosen file, if the file's data is in more than one partition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 173336"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFileNameOnes.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((,asyoulikeit),1)\n",
      "((AS,asyoulikeit),1)\n",
      "((YOU,asyoulikeit),1)\n",
      "((LIKE,asyoulikeit),1)\n",
      "((IT,asyoulikeit),1)\n",
      "((DRAMATIS,asyoulikeit),1)\n",
      "((PERSONAE,asyoulikeit),1)\n",
      "((DUKE,asyoulikeit),1)\n",
      "((SENIOR,asyoulikeit),1)\n",
      "((living,asyoulikeit),1)\n"
     ]
    }
   ],
   "source": [
    "wordFileNameOnes.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We asked for results, so we forced Spark to run a job to compute results. Spark pipelines, like `iiFirstPass1` are _lazy_; nothing is computed until we ask for results. It's useful when learning to print some data to better understand what happening. Just be aware of the extra overhead of running more Spark jobs.\n",
    "\n",
    "Depending on which 10 records Spark selected, you might see \"\" (blank) as a word, e.g., this is one of the records:\n",
    "\n",
    "```\n",
    "((,asyoulikeit),1)\n",
    "```\n",
    "Also, some words are all capital letters:\n",
    "\n",
    "```\n",
    "((DRAMATIS,asyoulikeit),1)\n",
    "```\n",
    "\n",
    "(You can see where these occur in the original files.) We'll filter out the blank word and capitalization later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's join all the unique `(word,fileName)` pairs together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[83] at reduceByKey at <console>:43"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniques = wordFileNameOnes.reduceByKey((count1, count2) => count1 + count2)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL you would use `GROUP BY` for this (including in Spark's [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html) API. However, in the `RDD` API, this is too expensive for our needs, because we don't care about the groups, the long list of repeated `(word,fileName)` pairs. We only care about how many elements are in the groups, that is their _size_. That's the purpose of the `1` in the tuples and the use of `RDD.reduceByKey`. It brings together all records with the same key, the unique `(word,fileName)` pairs, and then applies the anonymous function we provide to \"reduce\" the values, the `1s`. We simply sum them up to compute the group counts.\n",
    "\n",
    "Note that this anonymous function takes two arguments, so we need parentheses around the argument list. Since this function fits on the same line, we used parentheses for `reduceByKey`, instead of braces.\n",
    "\n",
    "> **Note:** All the `*ByKey` methods operate on two-element tuples and treat the first element as the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are there? Let's see a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 27276"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((dexterity,merrywivesofwindsor),1)\n",
      "((crest,asyoulikeit),1)\n",
      "((whole,comedyoferrors),2)\n",
      "((lamb,muchadoaboutnothing),2)\n",
      "((force,muchadoaboutnothing),2)\n",
      "((letter,merrywivesofwindsor),19)\n",
      "((blunt,tamingoftheshrew),3)\n",
      "((bestow,asyoulikeit),1)\n",
      "((rear,midsummersnightsdream),1)\n",
      "((crossing,tamingoftheshrew),1)\n",
      "((wronged,merrywivesofwindsor),4)\n",
      "((S,tamingoftheshrew),10)\n",
      "((HIPPOLYTA,midsummersnightsdream),19)\n",
      "((revolve,twelfthnight),1)\n",
      "((er,merrywivesofwindsor),11)\n",
      "((renown,asyoulikeit),1)\n",
      "((cubiculo,twelfthnight),1)\n",
      "((All,twelfthnight),3)\n",
      "((power,loveslabourslost),8)\n",
      "((Albeit,asyoulikeit),1)\n",
      "((lips,tamingoftheshrew),3)\n",
      "((upshot,twelfthnight),1)\n",
      "((approach,midsummersnightsdream),4)\n",
      "((mean,muchadoaboutnothing),5)\n",
      "((embossed,asyoulikeit),1)\n",
      "((varnish,loveslabourslost),2)\n",
      "((Apollo,midsummersnightsdream),1)\n",
      "((spangled,midsummersnightsdream),1)\n",
      "((gentlemen,comedyoferrors),1)\n",
      "((Rebuke,loveslabourslost),1)\n"
     ]
    }
   ],
   "source": [
    "uniques.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect from a `GROUP BY`-like statement, the number of records is smaller than before.\n",
    "\n",
    "We want our final keys to be the words themselves, so let's restructure the tuples from `((word,fileName),count)` to `(word,(fileName,count))`. Now, we'll still output two-element, key-value tuples, but the `word` will be the key and the `(fileName,count)` tuple will be the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val words = uniques.map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nested tuple methods, e.g., `_1._2`, are hard to read, making the logic here somewhat obscure. We'll see a beautiful and elegant alternative shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use an actual `group by` operation, because we now need the \"groups\". Calling `RDD.groupByKey` uses the first tuple element and brings together all occurrences of the unique words. Next, we'll sort the result by word, ascending alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[88] at sortByKey at <console>:47"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordGroups = words.groupByKey.sortByKey(ascending = true)\n",
    "wordGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"group\" is actually a Scala [Iterable](http://www.scala-lang.org/api/current/index.html#scala.collection.Iterable), i.e., an abstraction for some sort of collection. (We're about to see that it's a Spark-defined, private collection called a `CompactBuffer`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 11951"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordGroups.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,CompactBuffer((tamingoftheshrew,1), (asyoulikeit,1), (merrywivesofwindsor,1), (comedyoferrors,1), (midsummersnightsdream,1), (twelfthnight,1), (loveslabourslost,1), (muchadoaboutnothing,1)))\n",
      "(A,CompactBuffer((loveslabourslost,78), (midsummersnightsdream,39), (muchadoaboutnothing,31), (merrywivesofwindsor,38), (comedyoferrors,42), (asyoulikeit,34), (twelfthnight,47), (tamingoftheshrew,59)))\n",
      "(ABOUT,CompactBuffer((muchadoaboutnothing,18)))\n",
      "(ACT,CompactBuffer((asyoulikeit,22), (comedyoferrors,11), (tamingoftheshrew,12), (loveslabourslost,9), (muchadoaboutnothing,17), (twelfthnight,18), (merrywivesofwindsor,23), (midsummersnightsdream,9)))\n",
      "(ADAM,CompactBuffer((asyoulikeit,16)))\n",
      "(ADO,CompactBuffer((muchadoaboutnothing,18)))\n",
      "(ADRIANA,CompactBuffer((comedyoferrors,85)))\n",
      "(ADRIANO,CompactBuffer((loveslabourslost,111)))\n",
      "(AEGEON,CompactBuffer((comedyoferrors,20)))\n",
      "(AEMELIA,CompactBuffer((comedyoferrors,16)))\n",
      "(AEMILIA,CompactBuffer((comedyoferrors,3)))\n",
      "(AEacides,CompactBuffer((tamingoftheshrew,1)))\n",
      "(AEgeon,CompactBuffer((comedyoferrors,7)))\n",
      "(AEgle,CompactBuffer((midsummersnightsdream,1)))\n",
      "(AEmilia,CompactBuffer((comedyoferrors,4)))\n",
      "(AEsculapius,CompactBuffer((merrywivesofwindsor,1)))\n",
      "(AGUECHEEK,CompactBuffer((twelfthnight,2)))\n",
      "(ALL,CompactBuffer((midsummersnightsdream,2), (tamingoftheshrew,2)))\n",
      "(AMIENS,CompactBuffer((asyoulikeit,16)))\n",
      "(ANDREW,CompactBuffer((twelfthnight,104)))\n",
      "(ANGELO,CompactBuffer((comedyoferrors,36)))\n",
      "(ANN,CompactBuffer((merrywivesofwindsor,1)))\n",
      "(ANNE,CompactBuffer((merrywivesofwindsor,27)))\n",
      "(ANTIPHOLUS,CompactBuffer((comedyoferrors,195)))\n",
      "(ANTONIO,CompactBuffer((muchadoaboutnothing,32), (twelfthnight,32)))\n",
      "(ARMADO,CompactBuffer((loveslabourslost,111)))\n",
      "(AS,CompactBuffer((asyoulikeit,24)))\n",
      "(AUDREY,CompactBuffer((asyoulikeit,18)))\n",
      "(Abate,CompactBuffer((midsummersnightsdream,1), (loveslabourslost,1)))\n",
      "(Abbess,CompactBuffer((comedyoferrors,2)))\n"
     ]
    }
   ],
   "source": [
    "wordGroups.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up these `CompactBuffers`. Let's convert each to a Scala [Vector](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Vector) (a collection with _O(1)_ performance for most operations), then sort it descending by count, so the locations that mention the corresponding word the _most_ appear _first_ in the list. Note we're using `Vector.sortBy`, not an `RDD` sorting method. It takes a function that accepts each collection element and returns something used to sort the collection. By returning `(-fileNameCountTuple2._2, fileNameCountTuple2)`, we effectively say, \"sort by the counts _descending_ first, then the file names.\"\n",
    "\n",
    "Finally, we take the result `Vector` and make a comma-separated string with the elements.\n",
    "\n",
    "What's `RDD.mapValues`? We could use `RDD.map`, but we aren't changing the keys (words), so rather than have to deal with those too, `mapValues` is a convenience method that just passes in the value part of the tuple and reconstructs new `(key,value)` tuples with the returned, new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiFirstPass2 = wordGroups.mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done! The number of records is the same as for `wordGroups` (do you understand why?), so let's just see see some of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,(asyoulikeit,1),(comedyoferrors,1),(loveslabourslost,1),(merrywivesofwindsor,1),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n",
      "(A,(loveslabourslost,78),(tamingoftheshrew,59),(twelfthnight,47),(comedyoferrors,42),(midsummersnightsdream,39),(merrywivesofwindsor,38),(asyoulikeit,34),(muchadoaboutnothing,31))\n",
      "(ABOUT,(muchadoaboutnothing,18))\n",
      "(ACT,(merrywivesofwindsor,23),(asyoulikeit,22),(twelfthnight,18),(muchadoaboutnothing,17),(tamingoftheshrew,12),(comedyoferrors,11),(loveslabourslost,9),(midsummersnightsdream,9))\n",
      "(ADAM,(asyoulikeit,16))\n",
      "(ADO,(muchadoaboutnothing,18))\n",
      "(ADRIANA,(comedyoferrors,85))\n",
      "(ADRIANO,(loveslabourslost,111))\n",
      "(AEGEON,(comedyoferrors,20))\n",
      "(AEMELIA,(comedyoferrors,16))\n",
      "(AEMILIA,(comedyoferrors,3))\n",
      "(AEacides,(tamingoftheshrew,1))\n",
      "(AEgeon,(comedyoferrors,7))\n",
      "(AEgle,(midsummersnightsdream,1))\n",
      "(AEmilia,(comedyoferrors,4))\n",
      "(AEsculapius,(merrywivesofwindsor,1))\n",
      "(AGUECHEEK,(twelfthnight,2))\n",
      "(ALL,(midsummersnightsdream,2),(tamingoftheshrew,2))\n",
      "(AMIENS,(asyoulikeit,16))\n",
      "(ANDREW,(twelfthnight,104))\n",
      "(ANGELO,(comedyoferrors,36))\n",
      "(ANN,(merrywivesofwindsor,1))\n",
      "(ANNE,(merrywivesofwindsor,27))\n",
      "(ANTIPHOLUS,(comedyoferrors,195))\n",
      "(ANTONIO,(muchadoaboutnothing,32),(twelfthnight,32))\n",
      "(ARMADO,(loveslabourslost,111))\n",
      "(AS,(asyoulikeit,24))\n",
      "(AUDREY,(asyoulikeit,18))\n",
      "(Abate,(loveslabourslost,1),(midsummersnightsdream,1))\n",
      "(Abbess,(comedyoferrors,2))\n"
     ]
    }
   ],
   "source": [
    "iiFirstPass2.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Looks reasonable. \n",
    "\n",
    "Next, we'll refine the code using a very powerful feature, _pattern matching_, which both makes the code more concise and easier to understand.\n",
    "\n",
    "Before we do that, try a few refinements on your own.\n",
    "\n",
    "**Exercises:**\n",
    "\n",
    "* Add a filter statement to remove the first entry for the blank word \"\". You could do this one of two ways, using another \"step\" with [RDD.filter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) (search the [Scaladoc page]((http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) for the `filter` method), _or_ using the similar Scala collections method, [scala.collection.Seq.filter](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq). Both versions take a _predicate_ function, one that returns `true` if the record should be _retained_ and `false` otherwise. Do you think one choice is better than the other? Why? Or, are they basically the same? Reasons might include code comprehension and performance of one over the other.\n",
    "* Convert all words to lower case. Calling `toLowerCase` on a string is all you need. Where's a good place to insert this logic?\n",
    "\n",
    "We'll implement both changes in subsequent refinements below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If you would prefer to make a copy of the code in a new cell, use the _Insert_ menu above. Or use the keyboard shortcuts `ESC` (escape key), followed by `A` for insert before or `B` for insert after. Then hit return to edit. Note the toolbar pop-down for the format of the cell. This cell you're reading is _Markdown_. Use _Code_ for source code (obviously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "We wrote a real program and we've already learned quite a bit of Scala. Let's improve it with one of my favorite Scala features, _pattern matching_.\n",
    "\n",
    "Here's our \"first pass\" version again for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiFirstPass1b = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here it is another implementation, this time using _pattern matching_, which I'll explain in a moment. \n",
    "\n",
    "I've also made two other changes. Recall that we have entries for empty words \"\" and also mixed capitalization. I've added fixes for these:\n",
    "* `filter(word => word.size > 0)` to remove the empty words. (In Spark and Scala collections, `filter` has the positive sense; what should be retained?) It's indicated by the comment `// #1`.\n",
    "* `word.toLowerCase` to convert all words to lower case uniformly, so that words like HAMLET, Hamlet, and hamlet in the original texts are treated as the same, since we're counting word occurrences. See comment `// #2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ii1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I added the filtering inside the function passed to `flatMap`. My choice reduces the number of output records from `flatMap` by at most one record per line, which won't have a significant impact on performance. Filtering itself adds some extra overhead. \n",
    "\n",
    "Also, the way Spark implements steps like `map`, `flatMap`, `filter`, it would incur about the same overhead if we used `RDD.filter` instead. Note that you could also do the filtering later in the pipeline, any step after `groupByKey`. However you implemented this above is probably fine. You could do performance profiling of the different options, but you may not notice a difference except for very large input data sets.\n",
    "\n",
    "Let's verify we get the same results, but with the two improvements described. We'll use Spark's [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html) for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ii1DF = sqlContext.createDataFrame(ii1).toDF(\"word\", \"locations_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%%dataframe` _cell magic_ provides a nice table layout display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>locations_counts</th></tr><tr><td>a</td><td>(loveslabourslost,507),(merrywivesofwindsor,494),(muchadoaboutnothing,492),(asyoulikeit,461),(tamingoftheshrew,445),(twelfthnight,416),(midsummersnightsdream,281),(comedyoferrors,254)</td></tr><tr><td>abandon</td><td>(asyoulikeit,4),(tamingoftheshrew,1),(twelfthnight,1)</td></tr><tr><td>abate</td><td>(loveslabourslost,1),(midsummersnightsdream,1),(tamingoftheshrew,1)</td></tr><tr><td>abatement</td><td>(twelfthnight,1)</td></tr><tr><td>abbess</td><td>(comedyoferrors,8)</td></tr><tr><td>abbey</td><td>(comedyoferrors,9)</td></tr><tr><td>abbominable</td><td>(loveslabourslost,1)</td></tr><tr><td>abbreviated</td><td>(loveslabourslost,1)</td></tr><tr><td>abed</td><td>(asyoulikeit,1),(twelfthnight,1)</td></tr><tr><td>abetting</td><td>(comedyoferrors,1)</td></tr></table>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe\n",
    "ii1DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore our new implementation. We start off as before, by calling `wholeTextFiles`:\n",
    "\n",
    "```scala\n",
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we pass to `flatMap` now looks like this:\n",
    "\n",
    "```scala\n",
    "flatMap { \n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "            filter(word => word.size > 0)                      // #1\n",
    "        val fileName = location.split(pathSep).last\n",
    "        words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "}.\n",
    "```\n",
    "\n",
    "Compared to the previous version (without the enhancements marked with the \\#1 and \\#2 comments):\n",
    "\n",
    "```scala\n",
    "flatMap { location_contents_tuple2 => \n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```    \n",
    "\n",
    "Instead of `location_contents_tuple2` a variable name for the whole tuple, we write `case (location, contents)`. The `case` keyword says we want to _pattern match_ on the object passed to the function. If it is a two-element tuple (and we know it always will be in this case), then _extract_ the first element and assign it to a variable named `location` and extract the second element and assign it to a variable named `contents`.\n",
    "\n",
    "Now, instead of accessing the location and content with the slighly obscure `location_contents_tuple2._1` and `location_contents_tuple2._2`, respectively, we use meaningful names, `location` and `contents`. The code becomes more concise and readable. \n",
    "\n",
    "We'll explore more pattern matching features as we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduceByKey` step is unchanged:\n",
    "\n",
    "```scala\n",
    "reduceByKey((count1, count2) => count1 + count2).\n",
    "```\n",
    "\n",
    "To be clear, this isn't a pattern-matching expression; there is no `case` keyword. It's just \"regular\" function that takes two arguments, for the two things we're adding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My favorite improvement is the next line:\n",
    "\n",
    "```scala\n",
    "map { \n",
    "    case ((word, fileName), count) => (word, (fileName, count)) \n",
    "}.\n",
    "```\n",
    "\n",
    "Compare it to the previous, obscure version:\n",
    "\n",
    "```\n",
    "map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}.\n",
    "```\n",
    "\n",
    "The new implementation makes it clear what we're doing; just shifting parentheses! That's all it takes to go from our `(word, fileName)` keys with `count` values to `word` keys and `(fileName, count)` values. Note that pattern matching works just fine with nested structures, like `((word, fileName), count)`.\n",
    "\n",
    "I hope you can appreciate how elegant and concise this expression is! Note how I thought of the next transformation I needed to do in preparation for the final group-by, to switch from `((word, fileName), count)` to `(word, (fileName, count))` and _I just wrote it down exactly as I pictured it!_\n",
    "\n",
    "Code like this makes writing Scala Spark code a sublime experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two expressions are unchanged:\n",
    "\n",
    "```scala\n",
    "groupByKey.\n",
    "sortByKey(ascending = true).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final `mapValues` now uses pattern matching to sort the `Vector` in each record:\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { \n",
    "        case (fileName, count) => (-count, fileName) \n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```\n",
    "\n",
    "Compared to the original version, it's again easier to read:\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Version\n",
    "By the way, we can write SQL queries to explore this data. First, instead of creating a string for the list of `(location,count)` pairs, which isn't really useful, let's \"unzip\" the collection into two Arrays, one for the locations and one for the counts. That way, if we ask for the first element of each array, we'll have nicely separate fields that work better with structured queries, as provided by Spark SQL.\n",
    "\n",
    "Here's is `ii1` rewritten with this change. The comments explain what we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    map {                         // Must use map now, because we'll reformat new records. \n",
    "      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "\n",
    "        // Use `Vector.unzip`, which returns a single, two element tuple, where each\n",
    "        // element is a collection, one for the locations and one for the counts. \n",
    "        // We use pattern matching to extract these two collections into variables.\n",
    "        val (locations, counts) = vect.unzip  \n",
    "        \n",
    "        // Lastly, we'll compute the total count across all locations and return \n",
    "        // a new record with all four fields. The `reduceLeft` method takes a function\n",
    "        // that knows how to \"reduce\" the collection down to a final value, working \n",
    "        // from the left.\n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n",
    "        \n",
    "        (word, totalCount, locations, counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiDF = sqlContext.createDataFrame(ii).toDF(\"word\", \"total_count\", \"locations\", \"counts\")\n",
    "iiDF.registerTempTable(\"inverted_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a SQL query that extracts the top location by count for each word, as well as the total count across all locations for the word. The Spark SQL dialect supports Hive SQL syntax for extracting elements from arrays, maps, and structs ([details](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-CollectionFunctions)). Here we access the first element (index zero) from each array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------+-----------+----------------+---------+\n",
       "|       word|total_count|    top_location|top_count|\n",
       "+-----------+-----------+----------------+---------+\n",
       "|          a|       3350|loveslabourslost|      507|\n",
       "|    abandon|          6|     asyoulikeit|        4|\n",
       "|      abate|          3|loveslabourslost|        1|\n",
       "|  abatement|          1|    twelfthnight|        1|\n",
       "|     abbess|          8|  comedyoferrors|        8|\n",
       "|      abbey|          9|  comedyoferrors|        9|\n",
       "|abbominable|          1|loveslabourslost|        1|\n",
       "|abbreviated|          1|loveslabourslost|        1|\n",
       "|       abed|          2|     asyoulikeit|        1|\n",
       "|   abetting|          1|  comedyoferrors|        1|\n",
       "+-----------+-----------+----------------+---------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "SELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count \n",
    "FROM inverted_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the output formatting for Toree's `%%SQL` \"cell magic\" is not configurable. The `%%DataFrame` magic handles variable width layout and also provides more display options. First, to see the options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%%dataframe [arguments]\n",
       "DATAFRAME_CODE\n",
       "\n",
       "DATAFRAME_CODE can be any numbered lines of code, as long as the\n",
       "last line is a reference to a variable which is a DataFrame.\n",
       "    Option    Description                       \n",
       "------    -----------                       \n",
       "--limit   The number of records to return   \n",
       "            (default: 10)                   \n",
       "--output  The type of the output: html, csv,\n",
       "            json (default: html)            \n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the previous query, with the addition of a `WHERE` clause, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val topLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word,  total_count, locations[0] AS top_location, counts[0] AS top_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val topTwoLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word, total_count, \n",
    "        locations[0] AS first_location,  counts[0] AS first_count\n",
    "        locations[1] AS second_location, counts[1] AS second_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")\n",
    "%%dataframe --limit 100\n",
    "topTwoLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>total_count</th><th>top_location</th><th>top_count</th></tr><tr><td>beloved</td><td>11</td><td>tamingoftheshrew</td><td>4</td></tr><tr><td>cloven</td><td>1</td><td>loveslabourslost</td><td>1</td></tr><tr><td>cloves</td><td>1</td><td>loveslabourslost</td><td>1</td></tr><tr><td>glove</td><td>3</td><td>loveslabourslost</td><td>2</td></tr><tr><td>glover</td><td>1</td><td>merrywivesofwindsor</td><td>1</td></tr><tr><td>gloves</td><td>5</td><td>merrywivesofwindsor</td><td>3</td></tr><tr><td>hate</td><td>22</td><td>midsummersnightsdream</td><td>9</td></tr><tr><td>hated</td><td>6</td><td>midsummersnightsdream</td><td>4</td></tr><tr><td>hateful</td><td>5</td><td>midsummersnightsdream</td><td>3</td></tr><tr><td>hates</td><td>5</td><td>asyoulikeit</td><td>2</td></tr><tr><td>hateth</td><td>1</td><td>midsummersnightsdream</td><td>1</td></tr><tr><td>love</td><td>662</td><td>loveslabourslost</td><td>121</td></tr><tr><td>loved</td><td>38</td><td>asyoulikeit</td><td>13</td></tr><tr><td>lovely</td><td>15</td><td>midsummersnightsdream</td><td>7</td></tr><tr><td>lover</td><td>33</td><td>asyoulikeit</td><td>14</td></tr><tr><td>lovers</td><td>31</td><td>midsummersnightsdream</td><td>17</td></tr><tr><td>loves</td><td>51</td><td>muchadoaboutnothing</td><td>10</td></tr><tr><td>lovest</td><td>8</td><td>tamingoftheshrew</td><td>3</td></tr><tr><td>loveth</td><td>2</td><td>loveslabourslost</td><td>1</td></tr><tr><td>unloved</td><td>1</td><td>midsummersnightsdream</td><td>1</td></tr><tr><td>whate</td><td>4</td><td>tamingoftheshrew</td><td>3</td></tr><tr><td>whatever</td><td>1</td><td>tamingoftheshrew</td><td>1</td></tr></table>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 100\n",
    "topLocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _natural language processing_ (NLP) expert might tell you that _love_, _loved_, _loves_, etc. are really the same word, because they are different conjugations of the verb _to love_. Similarly, should _gloves_ (plural) and _glove_ (singular) be handled differently?\n",
    "\n",
    "What we really should do is extract the _stems_ of these words and use those instead. NLP toolkits handle this _stemming_ for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a useful `show` method on `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+---------+\n",
      "|   word|total_count|        top_location|top_count|\n",
      "+-------+-----------+--------------------+---------+\n",
      "|beloved|         11|    tamingoftheshrew|        4|\n",
      "| cloven|          1|    loveslabourslost|        1|\n",
      "| cloves|          1|    loveslabourslost|        1|\n",
      "|  glove|          3|    loveslabourslost|        2|\n",
      "| glover|          1| merrywivesofwindsor|        1|\n",
      "| gloves|          5| merrywivesofwindsor|        3|\n",
      "|   hate|         22|midsummersnightsd...|        9|\n",
      "|  hated|          6|midsummersnightsd...|        4|\n",
      "|hateful|          5|midsummersnightsd...|        3|\n",
      "|  hates|          5|         asyoulikeit|        2|\n",
      "| hateth|          1|midsummersnightsd...|        1|\n",
      "|   love|        662|    loveslabourslost|      121|\n",
      "|  loved|         38|         asyoulikeit|       13|\n",
      "| lovely|         15|midsummersnightsd...|        7|\n",
      "|  lover|         33|         asyoulikeit|       14|\n",
      "| lovers|         31|midsummersnightsd...|       17|\n",
      "|  loves|         51| muchadoaboutnothing|       10|\n",
      "| lovest|          8|    tamingoftheshrew|        3|\n",
      "| loveth|          2|    loveslabourslost|        1|\n",
      "|unloved|          1|midsummersnightsd...|        1|\n",
      "+-------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topLocations.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, it truncates column widths and only prints 20 rows. You can override that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------------------+---------+\n",
      "|word    |total_count|top_location         |top_count|\n",
      "+--------+-----------+---------------------+---------+\n",
      "|beloved |11         |tamingoftheshrew     |4        |\n",
      "|cloven  |1          |loveslabourslost     |1        |\n",
      "|cloves  |1          |loveslabourslost     |1        |\n",
      "|glove   |3          |loveslabourslost     |2        |\n",
      "|glover  |1          |merrywivesofwindsor  |1        |\n",
      "|gloves  |5          |merrywivesofwindsor  |3        |\n",
      "|hate    |22         |midsummersnightsdream|9        |\n",
      "|hated   |6          |midsummersnightsdream|4        |\n",
      "|hateful |5          |midsummersnightsdream|3        |\n",
      "|hates   |5          |asyoulikeit          |2        |\n",
      "|hateth  |1          |midsummersnightsdream|1        |\n",
      "|love    |662        |loveslabourslost     |121      |\n",
      "|loved   |38         |asyoulikeit          |13       |\n",
      "|lovely  |15         |midsummersnightsdream|7        |\n",
      "|lover   |33         |asyoulikeit          |14       |\n",
      "|lovers  |31         |midsummersnightsdream|17       |\n",
      "|loves   |51         |muchadoaboutnothing  |10       |\n",
      "|lovest  |8          |tamingoftheshrew     |3        |\n",
      "|loveth  |2          |loveslabourslost     |1        |\n",
      "|unloved |1          |midsummersnightsdream|1        |\n",
      "|whate   |4          |tamingoftheshrew     |3        |\n",
      "|whatever|1          |tamingoftheshrew     |1        |\n",
      "+--------+-----------+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topLocations.show(numRows = 40, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we used _named parameters_, `(numRows = 40, truncate = false)`, for legibility, but this is optional in Scala. You can also use it to write the arguments in any order you want, not just declaration order. In this case, we could have just written `(40, false)`, but then you would rightly wonder what `false` means in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Modify the query to return the top two locations and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, try writing other queries. Edit the query in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "|word       |total_count|locations                                                                                                                                       |counts                                  |\n",
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "|a          |3350       |[loveslabourslost, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, tamingoftheshrew, twelfthnight, midsummersnightsdream, comedyoferrors]|[507, 494, 492, 461, 445, 416, 281, 254]|\n",
      "|abandon    |6          |[asyoulikeit, tamingoftheshrew, twelfthnight]                                                                                                   |[4, 1, 1]                               |\n",
      "|abate      |3          |[loveslabourslost, midsummersnightsdream, tamingoftheshrew]                                                                                     |[1, 1, 1]                               |\n",
      "|abatement  |1          |[twelfthnight]                                                                                                                                  |[1]                                     |\n",
      "|abbess     |8          |[comedyoferrors]                                                                                                                                |[8]                                     |\n",
      "|abbey      |9          |[comedyoferrors]                                                                                                                                |[9]                                     |\n",
      "|abbominable|1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abbreviated|1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abed       |2          |[asyoulikeit, twelfthnight]                                                                                                                     |[1, 1]                                  |\n",
      "|abetting   |1          |[comedyoferrors]                                                                                                                                |[1]                                     |\n",
      "|abhominable|1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abhor      |5          |[asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, muchadoaboutnothing]                                                       |[1, 1, 1, 1, 1]                         |\n",
      "|abhors     |2          |[twelfthnight]                                                                                                                                  |[2]                                     |\n",
      "|abide      |5          |[merrywivesofwindsor, midsummersnightsdream]                                                                                                    |[3, 2]                                  |\n",
      "|abides     |1          |[muchadoaboutnothing]                                                                                                                           |[1]                                     |\n",
      "|ability    |2          |[muchadoaboutnothing, twelfthnight]                                                                                                             |[1, 1]                                  |\n",
      "|abject     |2          |[comedyoferrors, tamingoftheshrew]                                                                                                              |[1, 1]                                  |\n",
      "|abjure     |1          |[midsummersnightsdream]                                                                                                                         |[1]                                     |\n",
      "|abjured    |2          |[tamingoftheshrew, twelfthnight]                                                                                                                |[1, 1]                                  |\n",
      "|able       |9          |[merrywivesofwindsor, midsummersnightsdream, asyoulikeit, comedyoferrors, tamingoftheshrew]                                                     |[4, 2, 1, 1, 1]                         |\n",
      "|aboard     |6          |[comedyoferrors, tamingoftheshrew]                                                                                                              |[5, 1]                                  |\n",
      "|abode      |1          |[tamingoftheshrew]                                                                                                                              |[1]                                     |\n",
      "|abominable |2          |[asyoulikeit, merrywivesofwindsor]                                                                                                              |[1, 1]                                  |\n",
      "|abortive   |1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abound     |1          |[midsummersnightsdream]                                                                                                                         |[1]                                     |\n",
      "|about      |108        |[muchadoaboutnothing, merrywivesofwindsor, twelfthnight, tamingoftheshrew, asyoulikeit, comedyoferrors, midsummersnightsdream, loveslabourslost]|[35, 27, 10, 9, 7, 7, 7, 6]             |\n",
      "|above      |22         |[merrywivesofwindsor, twelfthnight, tamingoftheshrew, loveslabourslost, asyoulikeit, comedyoferrors, muchadoaboutnothing]                       |[6, 6, 4, 3, 1, 1, 1]                   |\n",
      "|abraham    |2          |[merrywivesofwindsor]                                                                                                                           |[2]                                     |\n",
      "|abridgement|1          |[midsummersnightsdream]                                                                                                                         |[1]                                     |\n",
      "|abroad     |3          |[loveslabourslost, tamingoftheshrew]                                                                                                            |[2, 1]                                  |\n",
      "|abrogate   |1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abruptly   |1          |[asyoulikeit]                                                                                                                                   |[1]                                     |\n",
      "|absence    |7          |[merrywivesofwindsor, asyoulikeit, comedyoferrors, loveslabourslost, midsummersnightsdream, twelfthnight]                                       |[2, 1, 1, 1, 1, 1]                      |\n",
      "|absent     |5          |[asyoulikeit, muchadoaboutnothing, tamingoftheshrew, twelfthnight]                                                                              |[2, 1, 1, 1]                            |\n",
      "|absolute   |1          |[merrywivesofwindsor]                                                                                                                           |[1]                                     |\n",
      "|abstinence |1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abstract   |1          |[merrywivesofwindsor]                                                                                                                           |[1]                                     |\n",
      "|abuse      |3          |[merrywivesofwindsor, twelfthnight]                                                                                                             |[2, 1]                                  |\n",
      "|abused     |11         |[twelfthnight, asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, midsummersnightsdream, muchadoaboutnothing, tamingoftheshrew]|[4, 1, 1, 1, 1, 1, 1, 1]                |\n",
      "|abuses     |2          |[asyoulikeit]                                                                                                                                   |[2]                                     |\n",
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sql1 = sqlContext.sql(\"\"\"\n",
    "    SELECT * FROM inverted_index\n",
    "\"\"\")\n",
    "sql1.show(40, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the \"Stop Words\"\n",
    "Did you notice that one record we saw above was for the word \"a\". Not very useful if you're using this data for text searching, _sentiment mining_, etc. So called _stop words_, like _a_, _an_, _the_, _he_, _she_, _it_, etc., could also be removed.\n",
    "\n",
    "Recall the `filter` logic we added to remove \"\", `word => word.size > 0`. We could replace it with `word => keep(word)`, where `keep` is a method that does any additional filtering we want, like removing stop words.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "* Implement the `keep(word: String):Boolean` method and change the `filter` function to use it. Have `keep` return `false` for a small, hard-coded list of stop words. (The solution is at the end of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Pattern Matching Syntax\n",
    "We've only scratched the surface of pattern matching. Let's explore it some more.\n",
    "\n",
    "Here's another anonymous function that uses this pattern matching form, which automatically drops empty files. (It could be a little smarter and look for files that have arbitrary whitespace, too.)\n",
    "\n",
    "```scala\n",
    "{\n",
    "    case (location, \"\") => \n",
    "        Array.empty[((String, String), Int)]  // Return an empty array\n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location.split(pathSep).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```\n",
    "\n",
    "So, you can have multiple `case` clauses, some of which might match on specific literal values (\"\" in this case) and others which are more general. Pattern matching is _eager_. The first successful match in the order as written will win. If you reversed the order here, the `case (location, \"\")` would never match and the compiler would throw an \"unreachable code\" warning.\n",
    "\n",
    "Note that you don't have to put the lines after the `=>` inside braces, `{...}` (although you can). The `=>` and `case` keywords (or the final `}`) are sufficient to mark these blocks. Also, for a single-expression block, like the first `case` clause, you can put the expression on the same line as the `case` clause. \n",
    "\n",
    "Finally, if none of the `case` clauses matches, then a [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError) exception is thrown. In our case, we _always_ know we'll have two-element tuples, so the examples so far are fine. Here's a final contrived example to illustrate what's possible, using a sequence of objects of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stuff = Seq(1, 3.14159, 2L, 4.4F, (\"one\", 1), (404F, \"boo\"), ((1, 2), 3, 4), \"hello\")\n",
    "\n",
    "stuff.foreach {\n",
    "    case i: Int               => println(s\"Found an Int:   $i\")\n",
    "    case l: Long              => println(s\"Found a Long:   $l\")\n",
    "    case f: Float             => println(s\"Found a Float:  $f\")\n",
    "    case d: Double            => println(s\"Found a Double: $d\")\n",
    "    case (anyType1, anyType2) => \n",
    "        println(s\"Found a two-element tuple with arbitrary elements: ($anyType1, $anyType2)\")\n",
    "    case ((anyType1, anyType2), _, anyType4) => \n",
    "        println(s\"Found a three-element tuple with (1st, 2nd) and 4th elements: ($anyType1, $anyType2) and $anyType4\")\n",
    "    case default              => println(s\"Found something else: $default\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes.\n",
    "* A literal like `1` is inferred to be `Int`, while `3.14159` is inferred to be `Double`. Add `L` or `F`, respectively, to infer `Long` and `Float` instead.\n",
    "* Note how we mixed specific type checking, e.g., `i: Int`, with more loosely-typed expressions, e.g., `(anyType1, anyType2)`, which expects a two-element tuple, but the element types are unconstrained.\n",
    "* All the words `i`, `l`, `f`, `d`, `anyType1`, `anyType2`, `anyType3`, and `default` are arbitrary variable names. Yes `default` is not a keyword, but an arbitrary choice. We could use anything we want.\n",
    "* The last `default` clause specifies a variable with no type information. Hence, it matches _anything_, which is why it must appear last. This last clause is the idiom to use when you don't know any information about what you're matching against. \n",
    "* If you want to match that something _exists_, but you don't need to bind it to a variable, then use `_`, as in the three-element tuple example.\n",
    "* The three-element tuple example also demonstrates that arbitrary nesting of types is supported.\n",
    "\n",
    "This pattern matching function `{ case firstCase => ...; case secondCase => ...; ... }` (yes, you could put expressions on the same line, separated by `;`) has a special name. It's called a _partial function_. All that means is that we only \"promise\" to accept arguments that match at least one of our `case` clauses, not any possible input. The other kind of anonymous function we've seen is a _total function_, to be precise. Recall we said that for those you could use either `(...)` or `{...}`, depending on the \"look\" you want. For _partial functions_, you must use `{...}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, recall that we used pattern matching with assignment:\n",
    "\n",
    "```scala\n",
    "val (locations, counts) = vect.unzip  \n",
    "```\n",
    "[Vector.unzip] returns a two-element tuple, where each element is a collection. We matched on that tuple and assigned each piece to a variable. Here's another contrived example, with nested tuple elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val (a, (b, (c1, c2), d)) = (\"A\", (\"B\", (\"C1\", \"C2\"), \"D\"))\n",
    "println(s\" $a, $b, $c1, $c2, $d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show one more useful example of pattern matching soon, with _case classes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala's Object Model\n",
    "Scala is a _hybrid_, object-oriented and functional programming language. The philosophy of Scala is that you exploit object orientation for encapsulation of details, but use functional programming for its logical precision, to implement those details. Most of what we've seen so far falls into the functional programming camp. Much of data manipulation and analysis is really Mathematics. Functional programming tries to stay close to how functions and values work in Mathematics.\n",
    "\n",
    "However, when writing non-trivial Spark programs, it's occasionally useful to exploit the object-oriented features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes vs. Instances\n",
    "Scala uses the same distinction between classes and instances. Classes are like _templates_ used to create instances. \n",
    "\n",
    "We've talked about the _types_ of things, like `word` is a `String` and `totalCount` is an `Int`. A class defines a _type_ in the same sense.\n",
    "\n",
    "Here is an example class that we might use to represent the inverted index records we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IIRecord1(\n",
    "    word: String, \n",
    "    total_count: Int, \n",
    "    locations: Array[String], \n",
    "    counts: Array[Int]) {\n",
    "    \n",
    "    /** CSV formatted string, but use [a,b,c] for the arrays */\n",
    "    override def toString: String = \n",
    "        s\"$word,$total_count,[${locations.mkString(\",\")}],[${counts.mkString(\",\")}]\"\n",
    "}\n",
    "\n",
    "new IIRecord1(\"hello\", 3, Array(\"one\", \"two\"), Array(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a class, the argument list after the class name is the argument list for the _primary constructor_. You can define secondary constructors, too, but it's not very common, in part for reasons we'll see shortly.\n",
    "\n",
    "Note that when you override a method that's defined in a parent class, like Java's `Object.toString`, Scala requires you to add the `override` keyword.\n",
    "\n",
    "We created an _instance_ of `IIRecord1` using `new`, just like in Java.\n",
    "\n",
    "Finally, as a side note, we've been using `Ints` (integers) all along for the various counts, but really for \"big data\", we should probably use `Longs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects\n",
    "\n",
    "I've been careful to use the word _instance_ for things we create from classes. That's because Scala has built-in support for the [Singleton Design Pattern](https://en.wikipedia.org/wiki/Singleton_pattern), i.e., when we only want one instance of a class. We use the `object` keyword. \n",
    "\n",
    "For example, in Java, you define a class with a `static void main(String[] arguments)` method as your entry point into your program. In Scala, you use an `object` to hold `main`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object MySparkJob {\n",
    "\n",
    "    val greeting = \"Hello Spark!\"\n",
    "    \n",
    "    def main(arguments: Array[String]) = {\n",
    "        println(greeting)\n",
    "        \n",
    "        // Create your SparkContext, etc., etc.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for classes, the name of the object can be anything you want. There is no `static` keyword in Scala. Instead of adding `static` methods and fields to classes as in Java, you put them in an object instead, as here.\n",
    "\n",
    "> **NOTE:** Because the Scala compiler must generate valid JVM byte code, these definitions are converted into the equivalent, Java-like static definitions in the output byte code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Classes\n",
    "Tuples are handy for representing records, but it would be nice if the fields were _named_, as well as _typed_. A good use for a class, like our `IIRecord1` above, us to represent this structure and give us named fields. Let's now refine that class definition to exploit some extra, very useful features in Scala.\n",
    "\n",
    "Consider the following definition of a _case class_ that represents our final record type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class IIRecord(\n",
    "    word: String, \n",
    "    total_count: Int = 0, \n",
    "    locations: Array[String] = Array.empty, \n",
    "    counts: Array[Int] = Array.empty) {\n",
    "\n",
    "    /** \n",
    "     * Different than our CSV output above, but see toCSV.\n",
    "     * Array.toString is useless, so format these ourselves. \n",
    "     */\n",
    "    override def toString: String = \n",
    "        s\"\"\"IIRecord($word, $total_count, [${locations.mkString(\", \")}], [${counts.mkString(\", \")}])\"\"\"\n",
    "    \n",
    "    /** CSV-formatted string, but use [a,b,c] for the arrays */\n",
    "    def toCSV: String = \n",
    "        s\"$word,$total_count,[${locations.mkString(\",\")}],[${counts.mkString(\",\")}]\"\n",
    "        \n",
    "    /** Return a JSON-formatted string for the instance. */\n",
    "    def toJSONString: String = \n",
    "        s\"\"\"{\n",
    "        |  \"word\":        \"$word\", \n",
    "        |  \"total_count\": $total_count, \n",
    "        |  \"locations\":   [${toJSONArrayString(locations)}],\n",
    "        |  \"counts\"       [${counts.mkString(\", \")}]\n",
    "        |}\n",
    "        |\"\"\".stripMargin\n",
    "\n",
    "    private def toJSONArrayString(array: Array[String]): String =\n",
    "        array.map(quote).mkString(\", \")\n",
    "    \n",
    "    private def quote(word: String): String = \"\\\"\" + word + \"\\\"\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I said that defining secondary constructors is not very common. In part, it's because I used a convenient feature, the ability to define default values for arguments. The default values mean that I can create instances without providing the arguments explicitly. Consider these two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "`toString` output:\n",
      "IIRecord(hello, 0, [], [])\n",
      "IIRecord(world!, 3, [one, two], [1, 2])\n",
      "\n",
      "`toJSONString` output:\n",
      "{\n",
      "  \"word\":        \"hello\", \n",
      "  \"total_count\": 0, \n",
      "  \"locations\":   [],\n",
      "  \"counts\"       []\n",
      "}\n",
      "\n",
      "{\n",
      "  \"word\":        \"world!\", \n",
      "  \"total_count\": 3, \n",
      "  \"locations\":   [\"one\", \"two\"],\n",
      "  \"counts\"       [1, 2]\n",
      "}\n",
      "\n",
      "\n",
      "`toCSV` output:\n",
      "hello,0,[],[]\n",
      "world!,3,[one,two],[1,2]\n"
     ]
    }
   ],
   "source": [
    "val hello = new IIRecord(\"hello\")\n",
    "val world = new IIRecord(\"world!\", 3, Array(\"one\", \"two\"), Array(1, 2))\n",
    "\n",
    "println(\"\\n`toString` output:\")\n",
    "println(hello)\n",
    "println(world)\n",
    "\n",
    "println(\"\\n`toJSONString` output:\")\n",
    "println(hello.toJSONString)\n",
    "println(world.toJSONString)\n",
    "\n",
    "println(\"\\n`toCSV` output:\")\n",
    "println(hello.toCSV)\n",
    "println(world.toCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added `toJSONString` to illustrate adding public (default visibility) and private methods to a class definition. If there are no methods or other values to define, I can omit the body complete; no empty `{}` required.\n",
    "\n",
    "Recall that the `override` keyword is required when redefining `toString`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, what about that `case` keyword? It tells the compiler to do several useful things for us, eliminating a lot of boilerplate that we would have to write for ourselves with other languages, like Java:\n",
    "\n",
    "1. Treat each constructor argument as an immutable (`val`) private field of the instance.\n",
    "1. Generate a public reader method for the field with the same name (e.g., `word`).\n",
    "1. Generate _correct_ implementations of the `equals` and `hashCode` methods, which people often implement incorrectly, as well as a default `toString` method. You can use your own definitions by adding them explicitly to the body. We did this for `toString`, so format the arrays in a nicer way than the default.\n",
    "1. Generate an `object IIRecord`, i.e., with the same name. The object is called the _companion object_.\n",
    "1. Generate a \"factory\" method in the companion object that takes the same argument list and instantiates an instance.\n",
    "1. Generate helper methods in the companion object that support pattern matching.\n",
    "\n",
    "Points 1 and 2 make each argument behave as if they are public, read-only fields of the instance, but they are actually implemented as described.\n",
    "\n",
    "Point 3 is important for correct behavior. Case class instances are often used as keys in [Maps](http://www.scala-lang.org/api/current/index.html#scala.collection.Map), Spark RDD and DataFrame methods, etc. In fact, you should _only_ use your case classes or Scala built-in types with the same properties, like `Int` and other number types, `String`, tuples, etc.\n",
    "\n",
    "For point 4, the _companion object_ is generated automatically by the compiler. It adds a \"factory\" method, point5 (discussed next), and methods that support pattern matching (point 6). You can explicitly define it yourself, if you want to add additional methods and fields to it. The compiler will still insert these other methods. However, see <a href=\"#Ambiguities\">Ambiguities with Companion Objects</a>. The bottom line is that you shouldn't define case classes in notebooks like this with extra methods in the companion object, due to parsing ambiguities.\n",
    "\n",
    "Point 5 means you actually rarely use `new` when creating instances. That is, the following are effectively equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val hello1 = new IIRecord(\"hello1\")\n",
    "val hello2 = IIRecord(\"hello2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually happens in the second case, without `new`? The \"factory\" method is actually called `apply`. In Scala, whenever you put an argument list after an object, as in the `hello2` case, Scala looks for an `apply` method to call. The arguments have to match the argument list for apply (number of arguments, types of arguments, accounting for default argument values, etc.). Hence, the `hello2` declaration is really this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val hello2b = IIRecord.apply(\"hello2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exploit this feature, too, in your other classes. We talked about word stemming above. Suppose you write a stemming library and declare an object for as the entry point. Here, I'll just do something simple; assume a trailing \"s\" means the word is a plural and remove it (a bad assumption...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "object stem {\n",
    "    def apply(word: String): String = word.replaceFirst(\"s$\", \"\") // insert real implementation!\n",
    "}\n",
    "\n",
    "println(stem(\"dog\"))\n",
    "println(stem(\"dogs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it looks like we're calling a function or method named `stem`. (Scala allows object and class names to start with a lower case letter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, point 6 means we can use our custom case classes in pattern matching expressions. I won't go into the methods actually implemented in the companion object and how the support pattern matching, but here is an example of how you would use it with our previously-defined `hello` and `world` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(hello with no occurrences., world! occurs 3 times: (one,1), (two,2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq(hello, world).map {\n",
    "    case IIRecord(word, 0, _, _) => s\"$word with no occurrences.\"\n",
    "    case IIRecord(word, cnt, locs, cnts) => \n",
    "        s\"$word occurs $cnt times: ${locs.zip(cnts).mkString(\", \")}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first case match ignored the locations and counts, because we know they will be empty arrays if the total count is 0! \n",
    "\n",
    "The second case match uses the `zip` method to put the locations and counts back together. Recall we used `unzip` to create the separate collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "So far, we've used Spark's RDD API. It's common to use case classes to represent the \"schema\" of records when working with [Datasets](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).\n",
    "\n",
    "A problem with [DataFrames](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) is the fact that the fields are untyped until you try to access them. `Datasets` restore the type safety of `RDDs` by using a case class as the definition of the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we need to explain one use of imports in Spark.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiDS = iiDF.as[IIRecord]\n",
    "iiDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iiDS.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Requires Spark 2.X:\n",
    "iiDS.select(iiDS(\"word\"), iiDS(\"total_count\"), sum(iiDS(\"counts\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Scala\n",
    "We've covered a lot already in this notebook, focusing on the first things you need to know about Scala. This section discusses additional details about Scala that you'll encounter relatively quickly. \n",
    "\n",
    "At this point, I suggest you create a new notebook and play with Spark using what you've learned, then come back to this section if you run into something you don't understand or when you're ready to learn more Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Everything in a Package\n",
    "In Java, `import foo.bar.*;` means import everything in the `bar` package.\n",
    "\n",
    "In Scala, `*` is actually a legal method name; think of defining multiplication for custom numeric types, like `Matrix`. Hence, this import statement in Scala would be ambigious. Therefore, Scala uses `_` instead of `*`, `import foo.bar._` (with the semicolon inferred)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traits\n",
    "_Traits_ are similar to Java 8 _interfaces_, used to define abstractions, but with the ability to provide \"default\" implementations of the methods declared. Unlike Java 8 interfaces, traits can also have fields representing \"state\" information about instances. There is a blury line between traits and _abstract classes_, again where some member methods or fields are not defined. In both cases, subtypes of a trait and/or abstract class must define any undefined members, or else it won't be allowed to construct instances of those subtypes.\n",
    "\n",
    "So, why have both traits and abstract classes? It's because Java only allows _single inheritance_; there can be only one _parent_ type, which is normally where you would use an abstract class, but Scala lets you \"mix in\" one or more additional traits (or use a trait as the parent class - yes, confusing). A great example \"mix in\" trait is one that implements logging. Any \"service\" type can mix in the logging trait to get \"instance\" access to this functionality. Schematically, it looks like the following:\n",
    "\n",
    "```scala\n",
    "// Assume severity `Level` and `Logger` types defined elsewhere...\n",
    "trait Logging {\n",
    "\n",
    "    def log(level: Level, message: String): Unit = logger.log(level, message)\n",
    "    \n",
    "    private logger: Logger = ...\n",
    "}\n",
    "\n",
    "abstract class Service {\n",
    "    def run(): Unit   // No body, so abstract!\n",
    "}\n",
    "\n",
    "class MyService extends Service with Logging {\n",
    "    def run(): Unit = {\n",
    "        log(INFO, \"Staring MyService...\")\n",
    "        ...\n",
    "        log(INFO, \"Finished MyService\")\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "`Unit` is Scala's equivalent to Java's `void`. It actually is a return value, unlike `void`, but we use it in the same sense of \"nothing useful will be returned\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranges\n",
    "What if you want some numbers between a start and end value? Use a [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), which has a nice literal syntax, e.g., `1 until 100`, `2 to 200 by 3`. \n",
    "\n",
    "The `Range` always includes the lower bound. Using `to` in a `Range` makes it _inclusive_ at the upper bound. Using `until` makes it _exclusive_ at the upper bound. Use `by` to specify a delta, which defaults to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 until 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 4, 7, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 to 10 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you need a small test data set to play with Spark, ranges can be convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,CompactBuffer((7,0), (14,0), (21,0), (28,0), (35,0), (42,0), (49,0)))\n",
      "(1,CompactBuffer((1,1), (8,1), (15,1), (22,1), (29,1), (36,1), (43,1), (50,1)))\n",
      "(2,CompactBuffer((2,2), (9,2), (16,2), (23,2), (30,2), (37,2), (44,2)))\n",
      "(3,CompactBuffer((3,3), (10,3), (17,3), (24,3), (31,3), (38,3), (45,3)))\n",
      "(4,CompactBuffer((4,4), (11,4), (18,4), (25,4), (32,4), (39,4), (46,4)))\n",
      "(5,CompactBuffer((5,5), (12,5), (19,5), (26,5), (33,5), (40,5), (47,5)))\n",
      "(6,CompactBuffer((6,6), (13,6), (20,6), (27,6), (34,6), (41,6), (48,6)))\n"
     ]
    }
   ],
   "source": [
    "val rdd7 = sc.parallelize(1 to 50).map(i => (i, i%7)).groupBy{ case (i, seven) => seven }.sortByKey()\n",
    "rdd7.take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkContext` also has a `range` method that effectively does the same thing as `sc.parallelize(some_range)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Interpreter (REPL) vs. Notebooks vs. Scala Compiler\n",
    "<a name=\"REPL\"></a>\n",
    "This notebook has been using a running Scala interpreter, a.k.a. _REPL_ (\"read, eval, print, loop\") to parse the Scala code. The Spark distribution comes with a `spark-shell` script that also lets you use the interpreter from the command line. Hence, you don't get the nice notebook UI.\n",
    "\n",
    "If you use `spark-shell`, there are a few behavior changes you should know about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using :paste Mode\n",
    "By default the Scala interpreter treats _each line_ you enter separately. This can cause surprises compared to how the Scala _compiler_ works, where it treats all the code in the same file in the same context.\n",
    "\n",
    "For example, the following code, where the expression continues on the second line, is handled successfully by the compiler, but not by the interpreter.\n",
    "\n",
    "```scala\n",
    "(1 to 100)\n",
    ".map(i => i*i)\n",
    "```\n",
    "\n",
    "the Interpreter thinks it finished parsing the expression when it hit the new line after the literal [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), `1 to 100`. It then throws an error on the opening `.` on the next line. On the other hand, the compiler keeps compiling, ignoring the new line in this case. \n",
    "\n",
    "This notebook also does the same thing as the \"raw\" interpreter, but in some cases, notebooks will use an interpeter command, `:paste` that tells the parser to parse all of the following lines together, just like the compiler would parse them, until the \"end of input\", which you indicate with `CTRL-D`. \n",
    "\n",
    "You can't experiment with it through this notebook, but your session would look something like this:\n",
    "\n",
    "```scala\n",
    "scala> :paste\n",
    "// Entering paste mode (ctrl-D to finish)\n",
    "\n",
    "(1 to 10)\n",
    ".map(i => i*i)\n",
    "<CTRL-D>\n",
    "\n",
    "// Exiting paste mode, now interpreting.\n",
    "\n",
    "res0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n",
    "\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguities with Companion Objects\n",
    "<a name=\"Ambiguities\"></a>\n",
    "As I wrote this notebook, I _wanted_ to demonstrate using the companion object `IIRecord` to define a method explicitly, but this leads to an ambiguity later on in the notebook if you attempt to use this method. The notebook gets confused between the case class and the object. \n",
    "\n",
    "While unfortunately, it's also true that once you start defining more involved case classes, with more than trivial methods and explicit additions to the default companion object, you should really define these types outside the notebook in a compiled library that you use within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala's Object Model\n",
    "Scala's Object Model, including its hierarchy of types, is similar to Java's, but with some interesting differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scala Type Hierarchy](images/ScalaTypeHierarchy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Java, all _reference types_ are descended from [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html). The name _reference type_ reflects the fact that the instances for all these types are allocated on the _heap_ and program variables are references to those heap locations.\n",
    "\n",
    "The primitives types, `int`, `long`, etc. are not considered part of the type hierarchy and are treated specially. This is in part a performance optimization, as instances of these types fit in CPU registers and the values are pushed onto stack frames. However, they have wrapper or \"boxed\" types, `Integer`, `Long`, etc., that are part of the type hierarchy, which you must use with Java's collections, for example (with the exception of arrays).\n",
    "\n",
    "Instead, Scala treats the primitives at the code level as basically the same as the reference types. You don't use `new Int(100)` for example, but you can call methods on `Int` instances. The code generated, in most cases, uses the optimized JVM primitives. \n",
    "\n",
    "Hence, the Scala type hierarchy defines a type [Any](http://www.scala-lang.org/api/current/#scala.Any) to be the a parent type of _both_ reference types and \"value\" types (for the primitives). Each of those subhierarchies have parent types, [AnyRef](http://www.scala-lang.org/api/current/#scala.AnyRef) is effectively the same as [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html), and [AnyVal](http://www.scala-lang.org/api/current/#scala.AnyVal) is the parent of the value types.\n",
    "\n",
    "Finally, for better \"soundness\", the Scala type system defines a real type to represent [Null](http://www.scala-lang.org/api/current/#scala.Null) and [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing). By defining `Null` to be the subtype of all reference types `AnyRefs` (but not `AnyVals`), it supports at the type level the (unfortunate) practice of using `null` for a reference value.\n",
    "\n",
    "However, `null` is not allowed for an `AnyVal`, so the true \"bottom type\" of the hierarchy is `Nothing`. Why is that useful. I'll explain in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try vs. Option vs. null\n",
    "<a name=\"TryOptionNull\"></a>\n",
    "\n",
    "Recall the signature of our `curl` method near the beginning of this notebook:\n",
    "\n",
    "```scala\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): Try[File] = ...\n",
    "```\n",
    "\n",
    "We explained briefly why we used `Try`. Let's explore it in more detail and also discuss an alternative, [Option](http://www.scala-lang.org/api/current/index.html#scala.Option)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `curl` is declared to return [util.Try[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Try), where `T` is `java.io.File` in this case, the reader knows that it might fail somehow. If so, the relevant exception will be returned wrapped in a subclass of `Try`, called [util.Failure[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Failure). However, if `curl` is successful, the `File` is returned wrapped in the other subclass of `Try`, [util.Success[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Success),\n",
    "\n",
    "Because of Scala's type safety, you must determine which result was returned and handle it appropriately. Consider the alternative that's popular in many languages, including Java. Here's a `curl2` declaration, that returns `File` instead:\n",
    "\n",
    "```scala\n",
    "def curl2(sourceURLString: String, targetDirectoryString: String): File = ...\n",
    "```\n",
    "\n",
    "Now, the return type tells you nothing about the possibility of failure. In Java, you might declare that one of several exceptions might be thrown; this isn't done in Scala. Instead, we use the return type to convey this information. So which is better? The problem with declaring `File` as the return type is that `curl2` has no choice but to return `null` if failure occurs but an exception isn't thrown. Unless we remember to check for `null`, we'll bet the infamous [NullPointerException](https://docs.oracle.com/javase/8/docs/api/java/lang/NullPointerException.html). So, using `Try[T]` prevents us from this loophole.\n",
    "\n",
    "Using `Try` rather than simply throwing an exception, means that `curl` always returns \"normally\", so the caller maintains full control of the call stack and special exception-catching logic isn't required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all the possible valid subclasses of `Try`? Really, there are only two, `Success` and `Failure`. It would be a mistake to allow a user to define other subtypes, like `MaybeCouldFailButWhoKnows`, because users of `Try` in pattern matching will always want to know that there are only two possibilities. Scala adds a keyword to enforce this logical behavior. `Try` is actually declared as follows:\n",
    "\n",
    "```scala\n",
    "sealed abstract class Try[+T] extends AnyRef\n",
    "```\n",
    "\n",
    "(`AnyRef` is the same as Java's `Object` supertype.) The `sealed` keyword says that _no_ subclasses of `Try` can be declared, _except_ in the same source file (which the library author wrote). Hence, users of `Try` can't declare their own subclasses, subverting the logical structure of this type hierarchy and other user's code that relies on this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have a situation where it makes no sense to involve an exception, but we want the same logically handling? This is where [Option[T]](http://www.scala-lang.org/api/current/index.html#scala.Option) comes in. \n",
    "\n",
    "`Option` is analogous to `Try`, it is a `sealed` abstract type with two possible subtypes:\n",
    "\n",
    "* [Some[T]](http://www.scala-lang.org/api/current/index.html#scala.None): I have a an instance of `T` for your, inside the `Some[T]`.\n",
    "* [None](http://www.scala-lang.org/api/current/index.html#scala.None): I don't have a value for your, sorry.\n",
    "\n",
    "Note that a hash map is a great example where I either have a value for a given key or I don't. Therefore, for Scala's [Map[K,V]](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) abstraction, where `K` is the key type and `V` is the value type, the `get` method has this signature:\n",
    "\n",
    "```scala\n",
    "def get(key: K): Option[V]\n",
    "```\n",
    "\n",
    "One again, you know from the type signature that you may or may not get a value instance for the input key, _and_ you **must** determine whether you got a `Some[V]` or a `None` as the result. Once again, we avoid return a `null` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we determine we which result we have? Let's look a few examples using `Option`. `Try` can be used similarly, with a few other ways available that we won't discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n",
      "3\n",
      "None\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "options.foreach { o =>\n",
    "    println(o.getOrElse(\"None\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n",
      "3\n",
      "None\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val options = Seq(None, Some(2), Some(3), None, Some(5))\n",
    "\n",
    "options.foreach {\n",
    "    case None    => println(None)\n",
    "    case Some(i) => println(i)  // Note how we extract the enclosed value.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to ignore the `None` values, use a _for comprehension_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for {\n",
    "    option <- options  // loop through the options, assign each to \"option\"\n",
    "    value  <- option   // extract the value from the Somes; if None, skip to the next option\n",
    "} println(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you might wonder how `None` is declared. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(hello)\n",
      "None\n",
      "Some(world!)\n"
     ]
    }
   ],
   "source": [
    "val opts: Seq[Option[String]] = Seq(Some(\"hello\"), None, Some(\"world!\"))\n",
    "opts.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, so it must mean that `None` is a valid subclass of `Option[String]`. That's actually true for all `Option[T]`. How can it be a valid subtype for _all_ of them? Here is how it's declared (omitting some details):\n",
    "\n",
    "```scala\n",
    "object None extends Option[Nothing] {...}\n",
    "\n",
    "```\n",
    "\n",
    "`None` carries no \"state\" information, because it doesn't wrap an instance like `Some[T]` does. Hence, we only need one instance for all uses, so it's declared as an object. Recall we mentioned above that the type system has a [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing) type, which is a subtype of all other types. Without diving into too many details, if a variable is of type `Option[String]`, then you can use an `Option[Nothing]` for it (i.e., the latter is a subtype of the former). This is why `Nothing` is useful, for cases like `None`, so we can have one instance of it, but still obey the rules of Scala's object-oriented type system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicits\n",
    "Scala has a powerful mechanism known as _implicits_ that is used in the Spark Scala API. Implicits are a big topic, so we'll focus just on the uses of it that are most important to understand.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Conversions\n",
    "We used `RDD` methods like `reduceByKey` above, but if you search for this method in the [RDD Scaladoc page](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), you won't find it. Instead it's defined in the [PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) type (along with all the other `*ByKey` methods). So, how can we use these methods as if they are defined for `RDD`??\n",
    "\n",
    "When the Scala compiler sees code calling a method that doesn't exist on the type, it looks for an _implicit conversion_ in the current scope, which can transform the instance into another type (i.e., by wrapping it), where the other type provides the needed method. The full signature inferred for the method as it's used must match the definition in the wrapping class.\n",
    "\n",
    "> **Note:** If you don't find a method in the [Spark Scaladocs](http://spark.apache.org/docs/latest/api/scala/index.html#package) for a type where you think it should be defined, look for related helper types with the method.\n",
    "\n",
    "Here's a small Scala example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// A sample class. Note it doesn't define a `toJSON` method:\n",
    "case class Person(name: String, age: Int = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:58: error: reference to Person is ambiguous;\n",
       "it is imported twice in the same scope by\n",
       "import INSTANCE.Person\n",
       "and import INSTANCE.Person\n",
       "    implicit class PersonToJSONString(person: Person) {\n",
       "                                              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To scope them, define implicit conversions within an object\n",
    "object implicits {\n",
    "\n",
    "    // `implicit` keyword tells the compiler to consider this conversion.\n",
    "    // It takes a `Person`, returning a new instance of `PersonToJSONString`,\n",
    "    // then resolves the invocation of `toJSON`.\n",
    "    implicit class PersonToJSONString(person: Person) {\n",
    "        def toJSON: String = s\"\"\"{\"name\": ${person.name}, \"age\": ${person.age}}\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import implicits._        // Now it is visible in the current scope.\n",
    "\n",
    "val p = Person(\"Dean Wampler\", 39)\n",
    "\n",
    "// Magic conversion to `PersonToJSONString`, then `toJSON` is called.\n",
    "p.toJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `RDDs`, the implicit conversions to `PairRDDFunctions` and other support types are handled for you. However, when you use Spark SQL and the [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) API, you'll need to import some of these conversions yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       word|total_count|\n",
      "+-----------+-----------+\n",
      "|          a|       3350|\n",
      "|    abandon|          6|\n",
      "|      abate|          3|\n",
      "|  abatement|          1|\n",
      "|     abbess|          8|\n",
      "|      abbey|          9|\n",
      "|abbominable|          1|\n",
      "|abbreviated|          1|\n",
      "|       abed|          2|\n",
      "|   abetting|          1|\n",
      "|abhominable|          1|\n",
      "|      abhor|          5|\n",
      "|     abhors|          2|\n",
      "|      abide|          5|\n",
      "|     abides|          1|\n",
      "|    ability|          2|\n",
      "|     abject|          2|\n",
      "|     abjure|          1|\n",
      "|    abjured|          2|\n",
      "|       able|          9|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val wtc = iiDF.select($\"word\", $\"total_count\")\n",
    "wtc.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column-reference syntax `$\"name\"` is implemented using the same mechanism in the Scala library that implements interpolated strings, `s\"$foo\"`. The `import sqlc.implicits._` makes it available. \n",
    "\n",
    "Note we imported something from an _instance_, rather than a package or type, as allowed in Java. This can be a useful feature in Scala, but it's also fragile, If you try `import sqlContext.implicits._`, you'll get a compiler error that a \"stable identifier\" is required. It turns out that doing the value assignment, `val sqlc = sqlContext` first meets this requirement. This is unique to the notebook environment. You normally won't see this problem if you use the `spark-shell` that comes with a Spark distribution or you write a Spark program and compile it with the Scala compiler.\n",
    "\n",
    "However, it would be better if Spark defined this `implicits` object on the `SQLContext` companion object instead of on instances of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, but unrelated to implicits, the `DataFrame` API lets you write SQL-like queries with a programmatic API. If you want to use built in functions like `min`, `max`, etc. on columns, you need the following `import` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `min`, `max`, `avg`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------------------+\n",
      "|min(total_count)|max(total_count)|  avg(total_count)|\n",
      "+----------------+----------------+------------------+\n",
      "|               1|            5208|16.651743683350947|\n",
      "+----------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val mma = iiDF.select(min(\"total_count\"), max(\"total_count\"), avg(\"total_count\"))\n",
    "mma.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other use of implicits worth understanding is _implicit arguments_ to methods. You will encounter this mechanism used when you read the Spark Scaladocs, even though you might never realize you're actually using it in your code!\n",
    "\n",
    "Recall I mentioned previously that you can define default values for method arguments. I just used it for the `age` argument for `Person`:\n",
    "\n",
    "```scala\n",
    "case class Person(name: String, age: Int = 0)\n",
    "```\n",
    "\n",
    "Sometimes we need something more sophisticated. For example, our library might have a group of methods that need a special argument passed to them that provides useful \"context\" information, but you don't want the user to be required to explicitly pass this argument every time. Here's an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait Add[T] {\n",
    "    def add(t1: T, t2: T): T\n",
    "}\n",
    "\n",
    "// Nested implicits so they don't conflict with the previous object implicits.\n",
    "object Adder {\n",
    "    object implicits {\n",
    "        implicit val intAdd = new Add[Int] { \n",
    "            def add(i1: Int, i2: Int): Int = i1+i2 \n",
    "        }\n",
    "        implicit val doubleAdd = new Add[Float] { \n",
    "            def add(d1: Double, d2: Double): Double = d1+d2 \n",
    "        }\n",
    "        implicit val stringAdd = new Add[String] { \n",
    "            def add(s1: String, s2: String): String = s1+s2 \n",
    "        }\n",
    "        // etc...\n",
    "    }\n",
    "}\n",
    "\n",
    "import Adder.implicits._\n",
    "\n",
    "def sum[T](ts: Seq[T])(implicit adder: Add[T]): T = {\n",
    "    ts.reduceLeft((t1, t2) => adder.add(t1, t2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int = 55"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(0 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float = 51.299995"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(0.0 to 5.5 by 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String = onetwothree"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Seq(\"one\", \"two\", \"three\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:83: error: could not find implicit value for parameter adder: Add[Char]\n",
       "              sum(Seq('a', 'b', 'c'))   // Characters\n",
       "                 ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Will fail, because there's not Add[Char] in scope:\n",
    "sum(Seq('a', 'b', 'c'))   // Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the implicit values `intAdd`, `doubleAdd`, and `stringAdd`, were used by the Scala interpreter for the `adder` argument in the second _argument list_ for `sum`. Note that you have to use a second argument list and all arguments there must be implicit. \n",
    "\n",
    "We could have avoided using implicit arguments if we defined custom `sum` methods for every type. That would have been simpler in this trivial case, but for nontrivial methods, the duplication is worth avoiding. Another advantage of this mechanism is that the user can define her own implicit `Add[T]` instances for domain types (say for example, `Money`) and they would \"just work\".\n",
    "\n",
    "The Scala collections API uses this mechanism to know how to construct a new collection of the same kind as the input collection when you use `map`, `flatMap`, `reduceLeft`, etc.\n",
    "\n",
    "Spark uses this pattern for [Encoders](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder) in the Spark SQL. Encoders are used to serialize values into the new, compact memory encoding introduced in the _Tungsten_ project (see for example, [here](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/)). Here's an example of creating a [Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), where the `toDS` method is first \"added\" to a Scala [Seq](http://www.scala-lang.org/api/current/#scala.collection.Seq) through an implicit conversion (specifically [SQLImplicits.localSeqToDatasetHolder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits), which is brought into scope by the `import sqlc.implicits._` statement earlier) and then `toDS` uses `Encoders` internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.Dataset[Int] = [value: int]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 to 10).toDS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "I appreciate the effort you put into studying this notebook. I hope you enjoyed it as much as I enjoyed writing it.\n",
    "\n",
    "Now you know the core elements of Scala that you need for using the Spark Scala API. I hope you can appreciate the power and elegance of Scala. I hope you will choose to use it for all of your data engineering tasks, not just for Spark. \n",
    "\n",
    "What about data science? There are many people who use Scala for data science in Spark, but today Python and R have much richer libraries for Mathematics and Machine Learning. That will change over time, but for now, you'll need to decide which language best fits your needs.\n",
    "\n",
    "As you use Scala, there will be more things you'll want to understand that we haven't covered, including common idioms, conventions, and tools used in the Scala community. The references at the beginning of the notebook will give you the information you need.\n",
    "\n",
    "Best wishes and thank you.\n",
    "\n",
    "[Dean Wampler, Ph.D.](mailto:dean@deanwampler.com)<br/>\n",
    "[@deanwampler](http://twitter.com/deanwampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Exercise Solutions\n",
    "Let's discuss the solutions to exercises that weren't already solved earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the Top Two Locations and Counts\n",
    "We used the `DataFrame` API to write a SQL query that returned the top location and count. Adding the next one is straightforward. What do you observe is returned when there isn't a second location and count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val topTwoLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word, total_count, \n",
    "        locations[0] AS first_location,  counts[0] AS first_count,\n",
    "        locations[1] AS second_location, counts[1] AS second_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "|    word|total_count|      first_location|first_count|     second_location|second_count|\n",
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "| beloved|         11|    tamingoftheshrew|          4|         asyoulikeit|           3|\n",
      "|  cloven|          1|    loveslabourslost|          1|                null|        null|\n",
      "|  cloves|          1|    loveslabourslost|          1|                null|        null|\n",
      "|   glove|          3|    loveslabourslost|          2|        twelfthnight|           1|\n",
      "|  glover|          1| merrywivesofwindsor|          1|                null|        null|\n",
      "|  gloves|          5| merrywivesofwindsor|          3|         asyoulikeit|           1|\n",
      "|    hate|         22|midsummersnightsd...|          9|         asyoulikeit|           6|\n",
      "|   hated|          6|midsummersnightsd...|          4|         asyoulikeit|           2|\n",
      "| hateful|          5|midsummersnightsd...|          3|    loveslabourslost|           1|\n",
      "|   hates|          5|         asyoulikeit|          2| merrywivesofwindsor|           1|\n",
      "|  hateth|          1|midsummersnightsd...|          1|                null|        null|\n",
      "|    love|        662|    loveslabourslost|        121|         asyoulikeit|         119|\n",
      "|   loved|         38|         asyoulikeit|         13| muchadoaboutnothing|          13|\n",
      "|  lovely|         15|midsummersnightsd...|          7|    tamingoftheshrew|           5|\n",
      "|   lover|         33|         asyoulikeit|         14|midsummersnightsd...|          10|\n",
      "|  lovers|         31|midsummersnightsd...|         17|         asyoulikeit|           6|\n",
      "|   loves|         51| muchadoaboutnothing|         10| merrywivesofwindsor|           9|\n",
      "|  lovest|          8|    tamingoftheshrew|          3| muchadoaboutnothing|           2|\n",
      "|  loveth|          2|    loveslabourslost|          1|    tamingoftheshrew|           1|\n",
      "| unloved|          1|midsummersnightsd...|          1|                null|        null|\n",
      "|   whate|          4|    tamingoftheshrew|          3|         asyoulikeit|           1|\n",
      "|whatever|          1|    tamingoftheshrew|          1|                null|        null|\n",
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topTwoLocations.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Recall you were asked to implement a `keep(word: String):Boolean` method that filters stop words.\n",
    "\n",
    "First, let's implement `keep`. You can find lists of stop words on the web. One such list for English can be found [here]( * From http://norm.al/2009/04/14/list-of-english-stop-words/). It includes many words that you might not consider stop words. Nevertheless, I'll just use a smaller list here.\n",
    "\n",
    "Note that I'll use a Scala [Set](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Set) to hold the stop words. We want _O(1)_ look-up performance. We just want to know if the word is in the set or not.\n",
    "\n",
    "I'll also add \"\", so I can remove the explicit test for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val stopWords = Set(\"\", \"a\", \"an\", \"and\", \"I\", \"he\", \"she\", \"it\", \"the\")\n",
    "\n",
    "// If the set contains the word, we return false - we don't want to keep it!\n",
    "// Note we haven't converted words to lower case before keep is called, \n",
    "// so we have to do it here. This means we'll do the conversion TWICE for\n",
    "// every word, here and in the original code passed to flatMap. Consider\n",
    "// how you might refacting the code to eliminate this duplication.\n",
    "\n",
    "def keep(word: String): Boolean = stopWords.contains(word.toLowerCase) == false  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is our program modified to use `keep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iiStopWords = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => keep(word))                  // <== here\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    map { \n",
    "      case (word, iterable) => \n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "        val (locations, counts) = vect.unzip  \n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)        \n",
    "        (word, totalCount, locations, counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(abandon,6,Vector(asyoulikeit, tamingoftheshrew, twelfthnight),Vector(4, 1, 1))\n",
      "(abate,3,Vector(loveslabourslost, midsummersnightsdream, tamingoftheshrew),Vector(1, 1, 1))\n",
      "(abatement,1,Vector(twelfthnight),Vector(1))\n",
      "(abbess,8,Vector(comedyoferrors),Vector(8))\n",
      "(abbey,9,Vector(comedyoferrors),Vector(9))\n",
      "(abbominable,1,Vector(loveslabourslost),Vector(1))\n",
      "(abbreviated,1,Vector(loveslabourslost),Vector(1))\n",
      "(abed,2,Vector(asyoulikeit, twelfthnight),Vector(1, 1))\n",
      "(abetting,1,Vector(comedyoferrors),Vector(1))\n",
      "(abhominable,1,Vector(loveslabourslost),Vector(1))\n",
      "(abhor,5,Vector(asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, muchadoaboutnothing),Vector(1, 1, 1, 1, 1))\n",
      "(abhors,2,Vector(twelfthnight),Vector(2))\n",
      "(abide,5,Vector(merrywivesofwindsor, midsummersnightsdream),Vector(3, 2))\n",
      "(abides,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(ability,2,Vector(muchadoaboutnothing, twelfthnight),Vector(1, 1))\n",
      "(abject,2,Vector(comedyoferrors, tamingoftheshrew),Vector(1, 1))\n",
      "(abjure,1,Vector(midsummersnightsdream),Vector(1))\n",
      "(abjured,2,Vector(tamingoftheshrew, twelfthnight),Vector(1, 1))\n",
      "(able,9,Vector(merrywivesofwindsor, midsummersnightsdream, asyoulikeit, comedyoferrors, tamingoftheshrew),Vector(4, 2, 1, 1, 1))\n",
      "(aboard,6,Vector(comedyoferrors, tamingoftheshrew),Vector(5, 1))\n",
      "(abode,1,Vector(tamingoftheshrew),Vector(1))\n",
      "(abominable,2,Vector(asyoulikeit, merrywivesofwindsor),Vector(1, 1))\n",
      "(abortive,1,Vector(loveslabourslost),Vector(1))\n",
      "(abound,1,Vector(midsummersnightsdream),Vector(1))\n",
      "(about,108,Vector(muchadoaboutnothing, merrywivesofwindsor, twelfthnight, tamingoftheshrew, asyoulikeit, comedyoferrors, midsummersnightsdream, loveslabourslost),Vector(35, 27, 10, 9, 7, 7, 7, 6))\n",
      "(above,22,Vector(merrywivesofwindsor, twelfthnight, tamingoftheshrew, loveslabourslost, asyoulikeit, comedyoferrors, muchadoaboutnothing),Vector(6, 6, 4, 3, 1, 1, 1))\n",
      "(abraham,2,Vector(merrywivesofwindsor),Vector(2))\n",
      "(abridgement,1,Vector(midsummersnightsdream),Vector(1))\n",
      "(abroad,3,Vector(loveslabourslost, tamingoftheshrew),Vector(2, 1))\n",
      "(abrogate,1,Vector(loveslabourslost),Vector(1))\n",
      "(abruptly,1,Vector(asyoulikeit),Vector(1))\n",
      "(absence,7,Vector(merrywivesofwindsor, asyoulikeit, comedyoferrors, loveslabourslost, midsummersnightsdream, twelfthnight),Vector(2, 1, 1, 1, 1, 1))\n",
      "(absent,5,Vector(asyoulikeit, muchadoaboutnothing, tamingoftheshrew, twelfthnight),Vector(2, 1, 1, 1))\n",
      "(absolute,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(abstinence,1,Vector(loveslabourslost),Vector(1))\n",
      "(abstract,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(abuse,3,Vector(merrywivesofwindsor, twelfthnight),Vector(2, 1))\n",
      "(abused,11,Vector(twelfthnight, asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, midsummersnightsdream, muchadoaboutnothing, tamingoftheshrew),Vector(4, 1, 1, 1, 1, 1, 1, 1))\n",
      "(abuses,2,Vector(asyoulikeit),Vector(2))\n",
      "(abusing,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(aby,2,Vector(midsummersnightsdream),Vector(2))\n",
      "(academe,1,Vector(loveslabourslost),Vector(1))\n",
      "(academes,2,Vector(loveslabourslost),Vector(2))\n",
      "(accent,5,Vector(loveslabourslost, asyoulikeit, midsummersnightsdream, twelfthnight),Vector(2, 1, 1, 1))\n",
      "(accept,5,Vector(tamingoftheshrew),Vector(5))\n",
      "(access,7,Vector(tamingoftheshrew, asyoulikeit, twelfthnight),Vector(5, 1, 1))\n",
      "(accidence,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(accident,2,Vector(muchadoaboutnothing, twelfthnight),Vector(1, 1))\n",
      "(accidentally,2,Vector(comedyoferrors, loveslabourslost),Vector(1, 1))\n",
      "(accidents,1,Vector(midsummersnightsdream),Vector(1))\n",
      "(accompany,2,Vector(midsummersnightsdream, tamingoftheshrew),Vector(1, 1))\n",
      "(accomplished,3,Vector(loveslabourslost, tamingoftheshrew, twelfthnight),Vector(1, 1, 1))\n",
      "(accompt,1,Vector(loveslabourslost),Vector(1))\n",
      "(accord,3,Vector(asyoulikeit, tamingoftheshrew),Vector(2, 1))\n",
      "(accordant,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(according,10,Vector(asyoulikeit, midsummersnightsdream, comedyoferrors, merrywivesofwindsor, tamingoftheshrew, twelfthnight),Vector(3, 3, 1, 1, 1, 1))\n",
      "(accordingly,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(accords,1,Vector(comedyoferrors),Vector(1))\n",
      "(accost,6,Vector(twelfthnight),Vector(6))\n",
      "(accosted,1,Vector(twelfthnight),Vector(1))\n",
      "(account,3,Vector(muchadoaboutnothing, tamingoftheshrew),Vector(2, 1))\n",
      "(accounted,2,Vector(loveslabourslost, twelfthnight),Vector(1, 1))\n",
      "(accoutrement,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(accoutrements,2,Vector(asyoulikeit, tamingoftheshrew),Vector(1, 1))\n",
      "(accusation,4,Vector(muchadoaboutnothing),Vector(4))\n",
      "(accusative,2,Vector(merrywivesofwindsor),Vector(2))\n",
      "(accusativo,1,Vector(merrywivesofwindsor),Vector(1))\n",
      "(accuse,3,Vector(muchadoaboutnothing, merrywivesofwindsor),Vector(2, 1))\n",
      "(accused,6,Vector(muchadoaboutnothing),Vector(6))\n",
      "(accusers,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(accusing,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(accustom,1,Vector(asyoulikeit),Vector(1))\n",
      "(accustomed,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(ace,3,Vector(midsummersnightsdream, loveslabourslost),Vector(2, 1))\n",
      "(ache,2,Vector(comedyoferrors, muchadoaboutnothing),Vector(1, 1))\n",
      "(acheron,1,Vector(midsummersnightsdream),Vector(1))\n",
      "(achieve,7,Vector(tamingoftheshrew, twelfthnight),Vector(4, 3))\n",
      "(achieved,1,Vector(tamingoftheshrew),Vector(1))\n",
      "(achiever,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(achilles,1,Vector(loveslabourslost),Vector(1))\n",
      "(acknowledge,2,Vector(comedyoferrors, muchadoaboutnothing),Vector(1, 1))\n",
      "(acorn,3,Vector(midsummersnightsdream, asyoulikeit),Vector(2, 1))\n",
      "(acquaint,4,Vector(asyoulikeit, muchadoaboutnothing),Vector(2, 2))\n",
      "(acquaintance,16,Vector(merrywivesofwindsor, twelfthnight, asyoulikeit, midsummersnightsdream, muchadoaboutnothing, tamingoftheshrew),Vector(4, 4, 3, 3, 1, 1))\n",
      "(acquainted,14,Vector(merrywivesofwindsor, comedyoferrors, asyoulikeit, tamingoftheshrew, loveslabourslost),Vector(6, 3, 2, 2, 1))\n",
      "(acquit,3,Vector(asyoulikeit, merrywivesofwindsor, twelfthnight),Vector(1, 1, 1))\n",
      "(acquittances,1,Vector(loveslabourslost),Vector(1))\n",
      "(acres,1,Vector(asyoulikeit),Vector(1))\n",
      "(across,2,Vector(comedyoferrors, twelfthnight),Vector(1, 1))\n",
      "(act,127,Vector(merrywivesofwindsor, asyoulikeit, twelfthnight, muchadoaboutnothing, tamingoftheshrew, comedyoferrors, loveslabourslost, midsummersnightsdream),Vector(26, 22, 21, 17, 12, 11, 9, 9))\n",
      "(actaeon,2,Vector(merrywivesofwindsor),Vector(2))\n",
      "(acting,1,Vector(twelfthnight),Vector(1))\n",
      "(action,14,Vector(tamingoftheshrew, loveslabourslost, merrywivesofwindsor, midsummersnightsdream, muchadoaboutnothing, twelfthnight, asyoulikeit),Vector(3, 2, 2, 2, 2, 2, 1))\n",
      "(actions,3,Vector(asyoulikeit, twelfthnight),Vector(2, 1))\n",
      "(active,1,Vector(muchadoaboutnothing),Vector(1))\n",
      "(actor,2,Vector(asyoulikeit, midsummersnightsdream),Vector(1, 1))\n",
      "(actors,5,Vector(midsummersnightsdream, loveslabourslost),Vector(4, 1))\n",
      "(acts,1,Vector(asyoulikeit),Vector(1))\n",
      "(acute,2,Vector(loveslabourslost),Vector(2))\n",
      "(ad,2,Vector(loveslabourslost, tamingoftheshrew),Vector(1, 1))\n"
     ]
    }
   ],
   "source": [
    "iiStopWords.take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, we now have `filter(word => keep(word))`, but note how we used `println` in the previous cell to see results. We can do something similar with `filter` and instead write `filter(keep)`. \n",
    "\n",
    "What does this mean exactly? It tells the compiler \"convert the _method_ `keep` to a _function_ and pass that to `filter`.\" This works because `keep` already does what `filter` wants, take a single string argument and return a boolean result.\n",
    "\n",
    "Passing `keep` is actually different than passing `word => keep(word)`, which is an _anonymous_ function that _calls_ keep. We are using `keep` as the function itself, rather than constructing a function that uses `keep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
